---
layout: default
title: Python machine learning:machine learning and deep learning with Python, scikit-learn, and TensorFlow 2
parent: books
date:   2021-11-10 20:28:05 -0400
categories: ml
nav_order : 1
---

---
{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
1. TOC
{:toc}
</details>

---

# Chapter 2 : Training Simple Machine Learning Algorithms for Classification

åœ¨è¿™ä¸€ç« ä¸­ä»‹ç»äº†æœºå™¨å­¦ä¹ ä¸­çš„ç¥ç»å…ƒçš„å‘å±•å†å²ã€‚åœ¨ä¸‹å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ç§ç¥ç»å…ƒã€‚

ç¬¬ä¸€ç§æ˜¯æ¨¡æ‹Ÿäººç±»ç¥ç»å…ƒç»†èƒçš„æ¿€æ´»æœºåˆ¶æœ€æ—©æå‡ºæ¥çš„ã€‚è¾“å…¥ç«¯æ¨¡æ‹Ÿäº†æ ‘çªï¼Œç„¶åæ•´åˆæ‰€æœ‰çš„ä¿¡å·ï¼Œä¸streshold è¿›è¡Œæ¯”è¾ƒï¼Œæ¥åˆ¤å®šclass label(1 or -1)ã€‚

ç¬¬äºŒç§æ”¹è¿›çš„åœ°æ–¹æ˜¯åœ¨net inputä¹‹åå¢åŠ äº†ä¸€ä¸ªactivation functionã€‚è¿™æ ·ä¸¤ç§ç¥ç»å…ƒåœ¨å¯¹weights è¿›è¡Œupdateæ—¶ï¼Œupdates çš„æ¥æºæ˜¯ä¸åŒçš„ã€‚ç¬¬ä¸€ç§ç›´æ¥æ¥æºäºoutputï¼Œç¬¬äºŒç§æ¥æºäºactivation functionã€‚å‰è€…æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œåè€…æ˜¯ä¸€ä¸ªfloat numberã€‚è¿™æ ·åè€…å¯ä»¥åˆ¤å®šåç§»æ­£å¸¸class label çš„ç¨‹åº¦ã€‚è€Œç¬¬ä¸€ç§ä¸å¯ä»¥ã€‚

![perceptron vs adaptive linear neuron](/assets/images/python_ml/perceptron.png)

åŒæ—¶è¿™ä¸€ç« è¿˜ä»‹ç»äº†ä¸ºä»€ä¹ˆä¼šæœ‰biasï¼Œåœ¨perceptronä¸­ä¸ºäº†ä½¿threshold=0.æˆ‘ä»¬è®¤ä¸ºæ·»åŠ äº†ä¸€ä¸ªbiasï¼Œå¹¶ä¸”biaså­˜æ”¾åœ¨weightsæ•°åˆ—çš„ç¬¬ä¸€ä¸ªã€‚

weightsåœ¨åˆå§‹åŒ–æ—¶ï¼Œå…¶åº”è¯¥æ˜¯å¾ˆå°çš„æ•°ï¼Œä½†ä¸èƒ½ä¸º0ï¼Œå¦‚æœä¸º0åˆ™ä¹¦æœ¬ä¸Šè¯´å…¶weightsçš„å˜åŒ–åªæ˜¯scalerçš„å˜åŒ–ï¼Œè€Œæ²¡æœ‰æ–¹å‘ä¸Šçš„å˜åŒ–ã€‚åŒæ—¶ä½¿ç”¨`w = np.random.norm(loc=0.0, scale=0.1, size = 1+ x.shape[1])`æ¥åˆå§‹åŒ–weightsã€‚

åœ¨å†™codeæ—¶åº”åŸ¹å…»è‰¯å¥½çš„documentã€‚åœ¨æœ¬ç« ä¸­å¯ä»¥å€Ÿé‰´ä¸€ä¸‹ï¼š

```md

â€œâ€â€œfunction or class definition,å…·ä½“æ˜¯å¹²ä»€ä¹ˆçš„

Parameters
------
p1 : type
    definition
p2 : type
    definition
    
Returns
-------
r1 : type
    definition
r2 : type

Examples
-------
give some simple examples

Other
-------
Description

â€œâ€â€œ

```


## weight update

`w += det_w`


![weights update](/assets/images/python_ml/perceptron_update.png)

ä»¥ä¸Šæ˜¯ç¬¬ä¸€ç§ç¥ç»å…ƒPerceptronçš„weightsè¿›è¡Œupdateæ—¶ï¼Œweightså˜åŒ–

![weights update](/assets/images/python_ml/ada_update.png)

ä»¥ä¸Šæ˜¯ç¬¬äºŒç§ç¥ç»å…ƒadaçš„weights è¿›è¡Œupdateæ—¶weightsçš„å˜åŒ–é‡ã€‚å…¶æ¥æºäºä»¥ä¸‹cost å‡½æ•°ã€‚åªéœ€è¦å¯¹wjè¿›è¡Œæ±‚åå¯¼å°±å¯ä»¥å¾—åˆ°ä»¥ä¸Šå…¬å¼ã€‚

![weights update](/assets/images/python_ml/ada_cost.png)

ä¸¤è€…çš„åŒºåˆ«å¦‚ä¸‹ï¼š
1. perceptron ï¼š æ¯ä¸€ä¸ªexample(row)å¯¹weightè¿›è¡Œæ›´æ–°ä¸€æ¬¡ï¼› adaï¼šå…¬å¼é‡Œé¢æœ‰æ±‚å’Œç¬¦å·ï¼Œå› æ­¤æ˜¯å¯¹æ‰€æœ‰çš„exampleéå†å®Œä¹‹åå†æ›´æ–°weightsã€‚åè€…å«åš`Batch Gradient descent`ã€‚
2. å¯¹äºperceptronï¼Œ`error = yi - y^i` å…¶ä¸­outputæ˜¯class labelï¼Œå³ 1 or -1ã€‚è€Œada error ä¸­çš„output æ˜¯ä¸€ä¸ªfloat numberã€‚
3. perceptronï¼šdeta_w ä¸­çš„xæ˜¯ä¸€ä¸ªrowä¸­çš„feturesã€‚adaï¼šæ˜¯ä¸€ä¸ªç‚¹ç§¯ã€‚

å¦‚æœæˆ‘ä»¬çš„æ•°æ®é‡å¾ˆå¤§ï¼Œé‚£ä¹ˆéå†å®Œæ‰€æœ‰çš„exampleså†æ›´æ–°weightsåˆ™ä¸å®é™…ã€‚å› æ­¤æˆ‘ä»¬ä¹Ÿå¯ä»¥æ¯ä¸€ä¸ªexample æ›´æ–°ä¸€æ¬¡weightsï¼Œè¿™å«åš`stochastic gradient descent`ã€‚

å¯¹äºlearning rate(eta)è¶…å‚æ•°ã€‚å¦‚æœå¤ªå¤§ï¼Œåˆ™ä¼šå‘ç”Ÿovershootï¼Œå³åœ¨minumnç‚¹æ¥å›çš„èµ°åŠ¨ã€‚å¦‚æœå¤ªå°ï¼Œåˆ™èšåˆå¤ªæ…¢ã€‚

å¯¹äºdataä¸­æ•°æ®æ•°é‡çº§ä¸ä¸€è‡´çš„æƒ…å†µï¼Œä¾‹å¦‚æœ‰çš„åˆ—ä¸º0.09ï¼Œæœ‰çš„åˆ—ä¸º19800ï¼Œé‚£ä¹ˆæ˜¾ç„¶ç›´æ¥ä½¿ç”¨ä¸¤è€…çš„æ•°æ®è¿›è¡Œparameter fitingã€‚åè€…å½±å“ä¼šå¾ˆå¤§ï¼Œè€Œå‰è€…å½±å“å°ã€‚ä¹Ÿä¼šå½±å“èšåˆé€Ÿåº¦ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œnormalizeã€‚å¸¸è§çš„normalizeä¸º `(data - mean)/std`ä½¿ç”¨numpy å¾ˆç®€å•ï¼š`(x-x.mean())/x.std()`


ä¸ºä»€ä¹ˆè¦æ·»åŠ activationï¼šè¿™æ˜¯å› ä¸ºactivationå¾—å‡ºçš„ç»“æœä¸true labelçš„errorå¯ä»¥è¡¡é‡ä¸ture label çš„è·ç¦»ç›¸å·®å¤šå°‘ã€‚è€Œä½¿ç”¨thresholdåçš„ç»“æœï¼Œåˆ™ä¸èƒ½å‡¸æ˜¾ä¸ä¹‹ç¨‹åº¦ã€‚åœ¨è¿™ä¸€ç« ä¸­ä½¿ç”¨çš„æ˜¯linear activationï¼Œå³ç›´æ¥è¾“å‡ºï¼Œæ²¡æœ‰å˜åŒ–ã€‚

ç¥ç»å…ƒçš„æ•°æ®æµåŠ¨è¿‡ç¨‹
1. net input ï¼š`net_input = np.dot(X,self.w_[1:]) + self.w_[0]`. self.w_[0] is the bias
2. activation function
3. compute the error : `error = true_labe - output`. here output came from activation. But if the perceptron nueron, then it is the final predicted label
4. get the predicted label by comparing the threshold
5. do the next example or next batch


# Chapter 3 : A Tour of Machine Learning Classifiers Using scikit-learn

The five main steps that are involved in training a supervised machine learning algorithm can be summarized as follows:
1. Selecting features and collecting labeled training examples.
2. Choosing a performance metric.
3. Choosing a classifier and optimization algorithm.
4. Evaluating the performance of the model.
5. Tuning the algorithm.

åœ¨è¿™ä¸€ç« ä¸­ä»‹ç»äº†sklearn ä¸­å‡ ä¸ªå¸¸ç”¨çš„åˆ†ç±»ç®—æ³•ï¼šlogic regressionï¼Œsvmï¼Œdecision tree, random forest, KNN(k nearest neighbours)ã€‚

è§£å†³çš„ç–‘æƒ‘æœ‰ï¼š
1. logic regresssion ï¼šsigmoid activation functionçš„ç”±æ¥ã€‚
2. å„ç®—æ³•çš„ä¼˜ç¼ºç‚¹ã€‚åé¢è¯¦ç»†è¯´æ˜
3. decision tree åˆ¤å®šnodeçš„è®¡ç®—è¿‡ç¨‹ï¼šé€šè¿‡IG(information gain)æ¥å†³å®šå¯¹å“ªä¸ªfeature è¿›è¡Œåˆ†å‰²
4. random forest æ˜¯åœ¨æ•´ä¸ªdataset ä¸­éšæœºæœ‰æ”¾å›çš„é€‰æ‹©ä¸€äº›examplesï¼Œç„¶åå†éšæœºé€‰æ‹©ä¸€äº›featuresæ¥build decision treeã€‚è€Œ sklearnä¸­é»˜è®¤æƒ…å†µä¸‹æ˜¯ç›´æ¥ä½¿ç”¨å…¨éƒ¨çš„datasetï¼Œç„¶åéšæœºé€‰æ‹©subfeatureæ¥è¿›è¡Œbuild treeç»„æˆrandom forestã€‚
5. overfitting and underfitting
6. regularization

## æ¦‚ç‡ï¼Œå‘ç”Ÿæ¯”(odds), å‡€è¾“å…¥(net_input) å’Œlogit å…¬å¼ä¹‹é—´çš„è”ç³»

é¦–å…ˆæ˜ç¡®å‡ ä¸ªæ„Ÿå¿µï¼Œå…·ä½“ç»†èŠ‚å¯å‚è€ƒ[csdn](https://blog.csdn.net/youyanyu3817/article/details/106279297)ï¼š
- æ¦‚ç‡ï¼šæ¦‚ç‡æ˜¯æŒ‡ä¸€ä¸ªäº‹ä»¶å‘ç”Ÿçš„å¯èƒ½æ€§
- å‘ç”Ÿæ¯”:å‘ç”Ÿæ¯”æŒ‡ä¸€ä»¶äº‹å‘ç”Ÿä¸å…¶ä¸å‘ç”Ÿçš„æ¦‚ç‡ä¹‹æ¯”ï¼Œ
- Logit :Logitæ˜¯ç»™å‘ç”Ÿæ¯”å–å¯¹æ•°ï¼Œå³log(Odds)ï¼Œå…¶ä¸­logæ˜¯è‡ªç„¶å¯¹æ•°ã€‚
- net_input = X\*w

å› ä¸ºæˆ‘ä»¬ä»ä¸Šä¸€ç« ä¸­äº†è§£åˆ°äº†ç¥ç»å…ƒä¸­çš„ä¿¡æ¯æµåŠ¨è¿‡ç¨‹ã€‚çŸ¥é“äº†net_inputã€‚åœ¨å¾—åˆ°net_inputä¹‹å‰çš„è¿‡ç¨‹åŸºæœ¬éƒ½æ˜¯ä¸€è‡´çš„ã€‚é‚£ä¹ˆåœ¨perceptronä¸­ç›´æ¥ä½¿ç”¨net_input ç»è¿‡thresholdçš„ç»“æœï¼ˆlabelï¼‰ä½œä¸ºerror çš„å‚æ•° Â·`error = true_labe - label`ï¼›è€Œåœ¨Ada ä¸­åˆ™æ˜¯ä½¿ç”¨linear activation functionã€‚å³ç›´æ¥ä½¿ç”¨net_inputæ¥è®¡ç®—error:`error = true_label - net_input`ã€‚

ç”±äºæœ€åä¸€ç§ç¥ç»å…ƒé‡‡å–çš„æ˜¯æ¦‚ç‡ä½œä¸ºè¯„åˆ¤ç»“æœã€‚å› æ­¤å¯¹äºäºŒåˆ†ç±»æ¨¡å‹æ¥è¯´ï¼Œç»“æœåœ¨[0,1]ä¹‹é—´ã€‚å› æ­¤æˆ‘ä»¬è¦ä½¿net_inputç»è¿‡activation function ä¹‹åç»“æœä¹Ÿåœ¨[0,1]ä¹‹é—´ã€‚

é‚£ä¹ˆä¸ºä»€ä¹ˆä¼šå¼•å…¥å‘ç”Ÿæ¯”å’Œlogitå‘¢ï¼Ÿæˆ‘æƒ³åˆ°çš„æ˜¯è¿™ä¸¤è€…ä¸æ¦‚ç‡æœ‰å…³ã€‚ç¬¬ä¸€ä½åƒèƒèŸ¹çš„äººå°è¯•äº†ä»¥ä¸‹æ–¹æ³•ï¼Œè§‰å¾—ä¸é”™ã€‚è®°ä½æˆ‘ä»¬çš„ç›®çš„æ˜¯è¦æ±‚æ¦‚ç‡ï¼Œä¸”ä¸net_inputæ‰¯ä¸Šå…³ç³»ã€‚

- å¯ä»¥è®© `p = net_input` å—ï¼Ÿ

ä¸å¯ä»¥ï¼Œå› ä¸ºè¿™æ ·çš„è¯net_inputçš„èŒƒå›´åªèƒ½æ§åˆ¶åœ¨[0,1]ï¼Œä¸æ–¹ä¾¿update weightsã€‚å¤ªçª„äº†ã€‚

- å¯ä»¥è®©`odds = net_input` å—ï¼Ÿ

ä¸å¯ä»¥ï¼Œå› ä¸ºå‘ç”Ÿæ¯”æœ‰ä¸å¯¹ç§°æ€§, å½“æ¦‚ç‡ä»0.1å¢åŠ åˆ°0.2ï¼Œæ—¶ï¼Œå‘ç”Ÿæ¯”å¢åŠ äº†0.139ï¼›å½“æ¦‚ç‡ä»0.8å¢åŠ åˆ°0.9æ—¶ï¼Œå‘ç”Ÿæ¯”å´å¢åŠ äº†5ï¼Œè™½ç„¶æ¦‚ç‡çš„å¢é‡ç›¸åŒï¼Œä½†å‘ç”Ÿæ¯”çš„å¢é‡å´å¤§å¤§çš„ä¸åŒã€‚å…¶ä»–ä¾‹å­å¯ä»¥å‚è€ƒ[csdn](https://blog.csdn.net/youyanyu3817/article/details/106279297) 

- å¯ä»¥è®©`logit = net_input` å—ï¼Ÿ

ä¸ºäº†è§£å†³oddsçš„ä¸å¯¹ç§°æ€§ï¼Œå¼•å…¥äº†logitã€‚å› ä¸ºlogå‡½æ•°å®¹æ˜“è®¡ç®—å’Œæ±‚å¯¼ã€‚å› æ­¤è®©`logit(p) = z`, here z = net_input = X\*w.

log(p/1-p) = z
- p/(1-p) = e^z
- p = e^z/(1+e^z)
- p = 1/(1+e^-z)

åˆ°è¿™é‡Œæˆ‘ä»¬æŠŠæ¦‚ç‡å’Œnet_inputè”ç³»åœ¨äº†ä¸€èµ·ï¼Œå¹¶ä¸”`1/(1+e^-z)`å°±æ˜¯activation functionã€‚

## é€šè¿‡activation function æ›´æ–°updates

![ada and logic regression difference](/assets/images/python_ml/ada_lr_nueron.png)

ä»å›¾ä¸­å¯ä»¥çœ‹å‡º ada ä¸ logic regression ä¹‹é—´çš„ä¸åŒä¹‹å¤„ï¼š
1. activation functionæ˜¯ä¸åŒçš„ã€‚ada æ˜¯linear activationï¼›logic regresionæ˜¯sigmoid activationã€‚
2. logic regression å¯ä»¥æ±‚å¾—å½“å‰class label çš„æ¦‚ç‡ã€‚

logic regression çš„cost functionæ˜¯æ±‚å¾—æœ€å¤§çš„likelihood. æ±‚deta_wçš„è¿‡ç¨‹å¦‚ä¸‹ï¼š
1. è·å¾—æœ€å¤§likelihoodï¼Œè¿™æ˜¯ä¸€ä¸ªä¹˜ç§¯å…¬å¼
2. æ±‚logè½¬åŒ–ä¸ºæ±‚å’Œå…¬å¼

ä¸ºä»€ä¹ˆè½¬åŒ–ä¸ºæ±‚å’Œå…¬å¼ï¼š
1. å› ä¸ºä¹˜ç§¯ä¼šå¯¼è‡´æ•°æ®æº¢å‡ºã€‚
2. æ±‚å¯¼æ–¹ä¾¿

## overfitting and underfitting

 If a model suffers from overfitting, we also say that the model has a high variance, 

Similarly, our model can also suffer from underfitting (high bias), which means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance
on unseen data.

overfitting : high variance
underfitting : high bias

## logic regression VS svm

logic regression æ›´å®¹æ˜“å—åˆ°outlier data çš„å½±å“ã€‚è€Œsvm å¯ä»¥é€šè¿‡è°ƒèŠ‚Cå€¼ æ¥æ”¹å˜svmçš„regularizationçš„ç¨‹åº¦ã€‚

## kernel SVM

å¦‚æœdata æ˜¯å¯ä»¥è¿›è¡Œçº¿æ€§åˆ’åˆ†çš„ï¼Œé‚£ä¹ˆä½¿ç”¨linear SVMå°±å¯ä»¥äº†ã€‚å¦‚æœä¸æ˜¯çº¿æ€§åˆ’åˆ†çš„(nonlinear sepratable),é‚£ä¹ˆå¯ä»¥ä½¿ç”¨kernel SVMè¿›è¡Œåˆ†å‰²ã€‚å…¶åŸºæœ¬æ€è·¯æ˜¯æŠŠåœ¨å½“å‰ç»´åº¦ä¸Šä¸å¯åˆ†çš„æ•°æ®è¿›è¡Œå‡ç»´ã€‚åœ¨é«˜ç»´ä¸Šå°±å¯ä»¥è½¬åŒ–æˆä¸ºçº¿æ€§å¯åˆ†äº†ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![kernel svm](/assets/images/python_ml/kernel_svm.png)



<img src="https://render.githubusercontent.com/render/math?math=IG(D_p,f) = I(D_p) - \sum_{j=1}^{m} \frac{N_j}{N_p} I(D_j)">

## regularization

The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter (weight) values. 

The most common form of regularization is so-called L2 regularization (sometimes also called L2 shrinkage or weight decay), which can be written as follows:

![r2 regularization](/assets/images/python_ml/r2_regularization.png)

![r2 regularization](/assets/images/python_ml/cost_lr.png)

1. ä¹‹æ‰€ä»¥å«åšweight decay æ˜¯å› ä¸ºåç§»é‡ä¸weight æœ‰å…³ã€‚
2. åŒæ—¶ä¸ºäº†åšregularizationä¹Ÿè¦è¿›è¡Œfeature normalizationï¼Œè¿™æ ·å„weightsä¹‹é—´æœ‰å¯æ¯”è¾ƒæ€§ã€‚


## Decision Tree

- å¦‚ä½•åˆ†å‰²æ¯ä¸ªnode

ä»¥ä¸‹å…¬å¼æ˜¯decision tree çš„ç›®æ ‡å‡½æ•°ã€‚å³æ¯æ¬¡åˆ†å‰²éƒ½ä¼šä½¿`IG(D,f)`æœ€å¤§ã€‚

<img src="https://render.githubusercontent.com/render/math?math=IG(D_p,f) = I(D_p) - \sum_{j=1}^{m} \frac{N_j}{N_p} I(D_j)">

Here, f is the feature to perform the split; `ğ·ğ‘` and `ğ·ğ‘—` are the dataset of the parent and jth child node; `I` is our impurity measure; `ğ‘ğ‘` is the total number of training examples at the parent node; and `Nj` is the number of example in the jth child node.

The information gain is simply the difference between the impurity of the parent node and the sum of the child node impuritiesâ€”the lower the impurities of the child nodes, the larger the information gain. 

ä¸ºäº†ç®€å•ï¼Œä¸€èˆ¬ä½¿ç”¨binary decision treeã€‚å› æ­¤ä»¥ä¸Šå…¬å¼ç®€åŒ–ä¸ºå¦‚ä¸‹å…¬å¼ã€‚åªæœ‰å·¦å³ä¸¤ä¸ªchildã€‚

<img src="https://render.githubusercontent.com/render/math?math=IG(D_p,f) = I(D_p) - \frac{N_{left}}{N_p} I(D_{left}) - \frac{N_{right}}{N_p} I(D_{right})">


### information gain in Decision Tree

åœ¨ä»¥ä¸Šå…¬å¼ä¸­çš„`I(D)`å³ä¸çº¯å‡€åº¦æ£€æµ‹(impurity measure)å¯ä»¥æœ‰ä»¥ä¸‹ä¸‰ç§æ–¹æ³•ï¼š

1. Gini impurity
2. Entropy
3. Classification error

- Entropy equation

<img src="https://render.githubusercontent.com/render/math?math=I_H(t) = - \sum_{i=1}^{c} p(i|t) \log_2p(i|t)
">

å‰ææ˜¯non-empty class `p(i|t) != 0`
Therefore, we can say that the entropy criterion attempts to maximize the mutual information in the tree.

- Gini impurity

The Gini impurity can be understood as a criterion to minimize the probability of misclassification:

<img src="https://render.githubusercontent.com/render/math?math=I_G(t) = \sum_{i=1}^{c} p(i|t)(1-p(i|t)) = 1 - \sum_{i=1}^{c}p(i|t)^2
">

Gini å¯ä»¥è®¤ä¸ºæ˜¯å°½é‡ä½¿è¯¯åˆ†ç±»çš„classæ¦‚ç‡æœ€å°ã€‚Gini ä¸Entropy åœ¨perfectly mixed çš„æƒ…å†µä¸‹ï¼Œå€¼æœ€å¤§ã€‚å¦‚æœä¸¤ä¸ªclassï¼Œæ¯ä¸ªclass çš„æ¦‚ç‡ä¸º0.5ï¼Œé‚£ä¹ˆGiniçš„å€¼æ˜¯`1-2*0.5^2=0.5`; entropy çš„ç»“æœæ˜¯1. ä½¿ç”¨Gini å’Œ Entropy çš„ç»“æœéƒ½å·®ä¸å¤šã€‚

- classification error

<img src="https://render.githubusercontent.com/render/math?math=I_E(t) = 1 - \max\{p(i|t)\}
">


classification error å¯ä»¥ç”¨äºå‰ªæï¼Œä½†æ˜¯å¹¶ä¸é€‚åˆç”¨äºæ ‘çš„å¢é•¿ã€‚å› ä¸ºå®ƒå¯¹æ¯ä¸ªnodesä¸­classæ¦‚ç‡çš„å˜åŒ–å¹¶ä¸æ•æ„Ÿã€‚è™½ç„¶probabilityæœ‰å˜åŒ–ï¼Œä½†æ˜¯å¾ˆå¯èƒ½ç»“æœä¸€è‡´ã€‚ä½†æ˜¯Giniå’ŒEntropyå´å¯ä»¥æŸ¥å‡ºè¿™äº›å˜åŒ–ï¼Œå¹¶ç»™å‡ºåˆç†çš„ç»“æœã€‚

ä»ä¸‹é¢çš„æ¡ˆä¾‹ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºä¸‰è€…çš„ä¸åŒä¹‹å¤„ï¼š

![classification error](/assets/images/python_ml/classification_error.png)

![Gini error](/assets/images/python_ml/gini_error.png)

![Entropy error](/assets/images/python_ml/entropy_error.png)


### å»ºç«‹Decision Tree

```python
from sklearn.tree import DecisionTreeClassifier
tree_model = DecisionTreeClassifier(criterion='gini',
                                   max_depth=4,
                                   random_state=1)
tree_model.fit(X_train,y_train)

from sklearn import tree
tree.plot_tree(tree_model)
plt.show()
```

> é€šè¿‡ä½¿ç”¨ `tree.plot_tree`æ¥æ˜¾ç¤ºtree çš„å†…éƒ¨æ¯ä¸ªnodeæ˜¯å¦‚ä½•åˆ’åˆ†çš„ã€‚

![decsion tree](/assets/images/python_ml/decsion_tree.png)

### Random Forest

The random forest algorithm can be summarized in four simple steps:

1. Draw a random bootstrap sample of size n (randomly choose n examples from the training dataset with replacement).
2. Grow a decision tree from the bootstrap sample. At each node:
a. Randomly select d features without replacement.
b. Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain.
3. Repeat the steps 1-2 k times.
4. Aggregate the prediction by each tree to assign the class label by majority vote. 

> In the step 2,  instead of evaluating all features to determine the best split at each node, we only consider a random subset of those.


- Advantage of Random Forest

1. we don't have to worry so much about choosing good hyperparameter values. We typically don't need to prune the random forest since the ensemble model is quite robust to noise from the individual decision trees.
2. The only parameter that we really need to care about in practice is the number of trees, k, (step 3) that we choose for the random forest.
3. Other parameters inclue the size, n, of the bootstrap sample (step 1), and the number of features, d, that are randomly chosen for each split (step 2.a), respectively.


### K-nearest neighbors â€“ a lazy learning algorithm

KNN is a lazy learning algorithm and don't have parameters.

KNN å¹¶ä¸é€‚ç”¨äºç»´åº¦å¾ˆé«˜çš„æ•°æ®ã€‚å› ä¸ºåœ¨é«˜ç»´åº¦ä¸ŠKNNçš„ç»“æœä¼šå˜å¾—å¾ˆç–æ¾ã€‚å› æ­¤å¦‚æœæ˜¯é«˜ç»´åº¦çš„æ•°æ®ï¼Œåº”è¯¥é¦–å…ˆè¿›è¡Œé™ç»´ã€‚

# chapter 4 : Building Good Training Datasets â€“ Data Preprocessing


## df.isna().sum()

ä½¿ç”¨df.isna().sum()æŸ¥çœ‹æ¯ä¸ªcolumnä¸­æœ‰å¤šå°‘missing valuesã€‚

## df.values

ä½¿ç”¨`df.values`å°†dataframeè½¬åŒ–ä¸ºnumpyã€‚sklearn ä¸­ä¸»è¦æ˜¯å¯¹numpyçš„æ•°æ®è¿›è¡Œå¤„ç†ã€‚ç°åœ¨sklearnä¸­å¾ˆå¤šfunctionéƒ½å¯ä»¥æ”¯æŒpandasã€‚æ‰€ä»¥å¦‚æœå¯ä»¥è½¬åŒ–ä¸ºnumpyåˆ™å°½é‡è½¬åŒ–ä¸ºnumpyåå†ä½¿ç”¨sklearnã€‚

## æ–¹æ³•ä¸€ï¼šç›´æ¥åˆ é™¤ [df.dorpna(axis=?)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)

æœ€ç®€å•ç²—æš´çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨`df.dropna`ç›´æ¥åˆ é™¤ã€‚

- å¦‚æœæƒ³åˆ é™¤å¸¦æœ‰NaNçš„rowsï¼š `df.dropna(axis=0)`
- åˆ é™¤å¸¦æœ‰NaNçš„columns ï¼š `df.dropna(axis=1)`
- only drop rows where all columns are NaN : `df.dropna(how='all')`
- drop rows that have fewer than 4 real values : `df.dropna(thresh=4)`
- only drop rows where NaN appear in specific columns (here: 'C'): `df.dropna(subset=['C'])`


## æ–¹æ³•äºŒï¼šæ’å€¼æ³• Imputing missing values

ç”±äºç›´æ¥åˆ é™¤æ•°æ®æœ‰ä»¥ä¸‹ç¼ºç‚¹ï¼š
- å¦‚æœåˆ é™¤å¤ªå¤šçš„æ•°æ®å¯¼è‡´æ•°é‡å¤ªå°‘ï¼Œéš¾äºåˆ†ææŒ–æ˜å‡ºæœ‰ç”¨çš„æ•°æ®ã€‚
- å¦‚æœåˆ é™¤äº†columnï¼Œè€Œæ­¤columnå¯èƒ½ä¸å¯¹classè¿›è¡Œåˆ†ç¦»å¾ˆé‡è¦ï¼Œå¯¼è‡´åˆ†ç±»ä¸å‡†ç¡®ã€‚

é‰´äºæ­¤ï¼Œæ›´å¸¸ç”¨çš„æ–¹æ³•æ˜¯ä½¿ç”¨æ’å€¼æ³•ã€‚å¸¸ç”¨çš„å·®å€¼æ–¹æ³•æ˜¯ä½¿ç”¨å‡å€¼(mean imputation)ã€‚

- ä½¿ç”¨sklearn ä¸­çš„`SimpleImputer`å¾ˆæ–¹ä¾¿å®Œæˆã€‚

```python
from sklearn.impute import SimpleImputer
import numpy as np
imr = SimpleImputer(missing_values=np.nan, strategy='mean')
imr = imr.fit(df.values)
imputed_data = imr.transform(df.values)
imputed_data
```

> [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) å¯¹numpyæ•°æ®è¿›è¡Œå¤„ç†ã€‚å› æ­¤ä½¿ç”¨`df.values`æŠŠdataframeè½¬åŒ–ä¸ºnumpy. strategyå‚æ•°é™¤äº†`mean`ä¹‹å¤–ï¼Œè¿˜æœ‰`median or most_frequent`ã€‚å½“æ•°æ®æ˜¯categorical feature æ—¶ä½¿ç”¨`most_frequent`æ˜¯éå¸¸æœ‰å¸®åŠ©çš„ã€‚


- ä½¿ç”¨[df.fillna(df.mean())](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) å¡«è¡¥ç¼ºå¤±å€¼


ä½¿ç”¨df.fillna(df.mean())æ¥å¡«è¡¥ç¼ºå¤±å€¼(å‡å€¼)

## Handle categorical missing values

ä»¥ä¸‹pythonä»£ç çš„æ¡ˆä¾‹å¦‚ä¸‹

```python
df = pd.DataFrame([
          ['green', 'M', 10.1, 'class2'],
           ['red', 'L', 13.5, 'class1'],
         ['blue', 'XL', 15.3, 'class2']])
df.columns = ['color','size','price','classlabel']
```

### ordinal and nominal features

- ordinal features : Ordinal features can be understood as categorical values that can be sorted or ordered. ä¾‹å¦‚è¡¬è¡«çš„size å¤§å°ï¼Œ XL > L > M.

- nominal features don't imply any order ï¼š è¡¬è¡«çš„é¢œè‰²æ˜¯nominal featureï¼Œæˆ‘ä»¬ä¸èƒ½è¯´red > greenã€‚

### ordinal features è½¬åŒ–

æˆ‘ä»¬å¯ä»¥è®¤ä¸ºè®¾å®šordinal features mappingæŠŠdata ä¸­çš„categorical featu è½¬åŒ–ä¸ºinteger valueã€‚

```python
size_mapping = {
    'XL':3,
    'L':2,
    'M':1
}
df['size'] = df['size'].map(size_mapping)
```

ä»¥ä¸Šçš„mappingæˆ‘ä»¬è®¤ä¸º`XL=L+1=M+2`è€Œå¾—å‡ºæ¥çš„ã€‚å¦‚æœæˆ‘ä»¬ä¸çŸ¥é“è¿™ç§relationshipï¼Œä½†æ˜¯æˆ‘ä»¬çŸ¥é“`XL>L>M`ã€‚æˆ‘ä»¬å¯ä»¥ç”¨å¦ä¸€ç§å½¢å¼çš„encodingã€‚æˆ‘ä»¬æŠŠ`>M or >L`çš„sizeè½¬åŒ–ä¸ºbinary valueã€‚

```python
df['x>M'] = df['size'].apply(lambda x:1 if x in {'L','XL'} else 0 )
df['x>L'] = df['size'].apply(lambda x:1 if x == 'XL' else 0)

```

### convert class label into intergers

å› ä¸ºsklearnå¯¹æ•°å­—å¤„ç†å¾ˆåœ¨è¡Œï¼Œå¯¹äºcategorical valueså¹¶ä¸å‹å¥½ã€‚å› æ­¤æˆ‘ä»¬æœ€å¥½æŠŠclass labelè½¬åŒ–ä¸ºintergeræ–¹ä¾¿machine learning modelè¿›è¡Œå¤„ç†ã€‚

- ä½¿ç”¨mapping

```python
class_mapping = {k:i for i,k in enumerate(np.unique(df['classlabel'].values))}

df['classlabel'].map(class_mapping)
```

- ä½¿ç”¨sklearnè‡ªå¸¦çš„LabelEncoder

```python
from sklearn.preprocessing import LabelEncoder
class_le = LabelEncoder()
y = class_le.fit_transform(df['classlabel'].values) # è½¬åŒ–ä¸ºinteger
class_le.inverse_transform(y) # è½¬åŒ–ä¸ºcategorical values
```

### one-hot encoding on nominal features

å¯¹äºnominal featuresï¼Œæˆ‘ä»¬ä¸èƒ½ä½¿ç”¨mapping æˆ–è€…LabelEncoderè½¬åŒ–æˆä¸ºintergerã€‚å› ä¸ºè¿™æ ·çš„è¯ï¼Œmachine learning model ä¼šè®¤ä¸ºå„integerä¹‹é—´å­˜åœ¨å¤§å°å…³ç³»ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦æŠŠæ¯ä¸ªnominal valueè½¬åŒ–ä¸ºä¸€ä¸ªdummy feature(binary value)ã€‚å³è¿™ä¸ªvalueæ˜¯å¦æ˜¯å­˜åœ¨çš„ã€‚

- æ–¹æ³•ä¸€ï¼šä½¿ç”¨sklearnè‡ªå¸¦çš„ [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=onehotencoder#sklearn.preprocessing.OneHotEncoder)

```python
from sklearn.preprocessing import OneHotEncoder
X = df.values

color_ohe = OneHotEncoder()

color_ohe.fit_transform(X[:,0].reshape(-1,1)).toarray()

```

> å› ä¸ºsklearnä¸­çš„function å¯¹numpyæ•°æ®è¿›è¡Œå¤„ç†ï¼Œå› æ­¤ç¬¬ä¸€æ­¥ä¾¿æ˜¯æŠŠdataframeçš„æ•°æ®è½¬åŒ–ä¸ºnumpyæ•°æ®(df.values)ã€‚`X[:,0].reshape(-1,1)` è¿™ä¸€æ­¥æ“ä½œæ˜¯æŠŠXä¸­çš„æŸä¸€åˆ—æå–å‡ºæ¥(æ­¤æ—¶æ˜¯rowçš„å½¢å¼)ï¼Œç„¶åå†è½¬åŒ–ä¸ºnX1çš„matrixå½¢å¼ï¼Œå› ä¸ºsklearnä¸­çš„function ä¸­çš„fit_transformåªå¯¹matrixè¿›è¡Œå¤„ç†ã€‚ä¸ºäº†æ˜¾ç¤ºç»“æœä½¿ç”¨`toarray()`

- æ–¹æ³•äºŒï¼šä½¿ç”¨sklearnè‡ªå¸¦çš„ [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html?highlight=columntransformer#sklearn.compose.ColumnTransformer)

```python
from sklearn.compose import ColumnTransformer

c_transf = ColumnTransformer([
    ('onehot',OneHotEncoder(),[0]),
    ('nothing','passthrough',[1,2])
])

c_transf.fit_transform(X).astype(float)
```

> The ColumnTransformer accepts a list of (name, transformer, column(s)) as above. æœ€åæˆ‘ä»¬ä½¿ç”¨astypeæŠŠnumpyçš„æ•°æ®è¿›è¡Œæ ¼å¼è½¬æ¢ï¼Œå°†objectæ ¼å¼è½¬åŒ–æˆä¸ºfloatæ ¼å¼ã€‚

- ä½¿ç”¨DataFrameè‡ªå¸¦çš„[get_dummies](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)method

```python
pd.get_dummies(df[['price', 'color', 'size']])
```

åœ¨ä¸Šé¢çš„è½¬åŒ–ä¸­æˆ‘ä»¬å¯¹æ¯ä¸€ä¸ªnominal vlaueéƒ½è¿›è¡Œäº†dummy binary valueè½¬åŒ–ã€‚è¿™æ ·å¯¼è‡´æˆ‘ä»¬è½¬åŒ–åçš„ç»“æœæ˜¯é«˜åº¦ç›¸å…³çš„ã€‚ä¾‹å¦‚å½“color_blue=0 and color_green=0ï¼Œé‚£ä¹ˆcolor_redä¸€å®šæ˜¯1.è¿™æ ·é«˜åº¦ç›¸å…³çš„matrixä¼šå¯¼è‡´matrixåœ¨è®¡ç®—æ—¶éå¸¸å›°éš¾(inverse)ã€‚å› æ­¤æˆ‘ä»¬è¦åœ¨dummy valuesä¸­åˆ é™¤ä¸€åˆ—ã€‚

```python
pd.get_dummies(df[['price','color','size']],drop_first=True)

color_ohe = OneHotEncoder(categories='auto', drop='first')
c_transf = ColumnTransformer([
    ('onehot',color_ohe,[0]),
    ('nothing','passthrough',[1,2])
])
c_transf.fit_transform(X).astype(float)
```

> åœ¨pd.get_dummiesä¸­æ·»åŠ å‚æ•°`drop_first=True`;åœ¨OneHotEncoderä¸­æ·»åŠ å‚æ•°`categories='auto', drop='first'`


## åˆ†å‰²æ•°æ®

ä½¿ç”¨sklearnè‡ªå¸¦çš„train_test_splitå¯¹æ•°æ®è¿›è¡Œåˆ†å‰²

```python
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y)
```

> åˆ†å‰²çš„æ¯”ä¾‹ä¸ºtrain:test = 7:3;Providing the class label array y as an argument to stratify ensures that both training and test datasets have the same class proportions as the original dataset.


## Bringing features onto the same scale

feature scaling æ˜¯éå¸¸é‡è¦çš„ä¸€æ­¥ã€‚é™¤äº† `Decision trees and random forests` è¿™ä¸¤ä¸ªalgorithmä¸éœ€è¦è¿›è¡Œfeature scalingä¹‹å¤–ï¼Œç»å¤§éƒ¨åˆ†machine learning algorithméœ€è¦feature scalingã€‚ä¾‹å¦‚ gradient descent optimization algorithm

`feature scaling çš„é‡è¦æ€§`: å¦‚æœä¸¤ä¸ªcolumn feature çš„scaleä¸åŒã€‚ä¸€ä¸ªæ˜¯1åˆ°10ï¼Œå¦ä¸€ä¸ªæ˜¯1 åˆ°1000.é‚£ä¹ˆåœ¨training çš„è¿‡ç¨‹ä¸­ï¼Œthe algorithmä¼šå¿™äºå¯¹scaleå¤§çš„é‚£ä¸ªfeatureçš„weights è¿›è¡Œupdateã€‚

feature scaling çš„ä¸¤ç§å½¢å¼ï¼š
- normalization:

å³ min-max scalingçš„å¦ä¸€ç§å½¢å¼ã€‚å¯ä»¥æŠŠæ•°æ®é™åˆ¶åœ¨[0,1]ä¹‹å†…ã€‚å› æ­¤å½“æˆ‘ä»¬éœ€è¦æŠŠæ•°æ®é™åˆ¶åœ¨æŸä¸€èŒƒå›´å†…æ—¶ï¼Œmin-max scaleæ˜¯éå¸¸æœ‰ç”¨çš„ã€‚

<img src="https://render.githubusercontent.com/render/math?math=x_{norm}^{(i)} = \frac{x^{(i)} - x_{min}}{x_{max} - x_{min}}
">

> Here, x^i is a particular example, x_min is the smallest value in the feature column, and x_max is the larget value

sklearn ä¸­çš„MinMaxScalerå¯¹æ•°æ®è¿›è¡Œmin-max scaling

```python
from sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)
```

- standardizationï¼š

<img src="https://render.githubusercontent.com/render/math?math=x_{std}^{(i)} = \frac{x^{(i)} - \mu_x}{\sigma_x}
">

> u_x is the sample mean of a particular feature colum, and \\sigma_x is the correspongding standard deviation.


standardization ä¸»è¦æ˜¯ç”¨äºmachine learning algorithmsï¼Œå°¤å…¶æ˜¯åœ¨optimization ç®—æ³•ä¸­ï¼Œä¾‹å¦‚ gradient descent.

- è¿™æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨weightsåˆå§‹åŒ–æ—¶å¾€å¾€æŠŠweightsåˆå§‹åŒ–ä¸ºéå¸¸æ¥è¿‘äº0çš„éšæœºæ•°ã€‚è¿™æ—¶æˆ‘ä»¬ä½¿ç”¨standardization æŠŠfeatureä¹Ÿè®¾ç½®æˆä¸weights parameteråŒæ ·çš„standard normal distribution(zero means and unit variance)ï¼Œè¿™æ ·`weightsä¼šå­¦ä¹ çš„æ›´å¿«`
- å¦ä¸€ä¸ªåŸå› æ˜¯standarization ä¿ç•™outliers çš„æœ‰ç”¨ä¿¡æ¯ï¼Œç›¸è¾ƒäºmin-max scalingï¼Œstandarizationå¯¹outliersä¸æ•æ„Ÿã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨sklearnä¸­çš„`StandardScaler` methodå¯¹features è¿›è¡Œstandardizationã€‚

```python
from sklearn.preprocessing import StandardScaler

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)
```

- [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)

å¦‚æœdatasetæ¯”è¾ƒå°ï¼Œä¸”æœ‰å¾ˆå¤šoutliersï¼Œåˆæˆ–è€…å‘ç”Ÿoverfittingï¼Œé‚£ä¹ˆRobustScaleræ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚

## Selecting meaningful features

å½“æˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡å‹æ—¶å‘ç”Ÿoverfittingï¼ŒåŸå› æ˜¯å½“å‰çš„æ¨¡å‹å¯¹ä¸å½“å‰çš„datasetæ¥è¯´å¤ªå¤æ‚äº†ï¼Œå¯¼è‡´æ¨¡å‹æœ‰å¾ˆé«˜çš„variance and has a high generalization error. 

Common solutions to reduce the generalization error are as follows(é˜»æ­¢overfittingçš„æ–¹æ³•):

- Collect more training data
- Introduce a penalty for complexity via regularization
- Choose a simpler model with fewer parameters
- Reduce the dimensionality of the data

ç¬¬ä¸€æ¡ï¼Œæ”¶é›†æ›´å¤šçš„æ•°æ®å¹¶ä¸å®ç”¨ã€‚
1. æˆ‘ä»¬ä¸çŸ¥é“æ˜¯å¦æ˜¯ç”±äºæ”¶é›†çš„feature nosie å¤ªå¤šå¯¼è‡´æ¨¡å‹å‘ç”Ÿhigh varianceï¼Œå› æ­¤æ”¶é›†æ›´å¤šçš„samplesä¹Ÿæ²¡æœ‰ç”¨ã€‚
2. æˆæœ¬å¤ªé«˜ï¼Œæˆ–è€…æ ¹æœ¬æ²¡æœ‰åŠæ³•æ”¶é›†æ›´å¤šçš„samplesã€‚

ä¸‹é¢æˆ‘ä»¬çœ‹ä¸€ä¸‹é€šè¿‡regularizationå’Œdimensionality reduction via feature selectionã€‚å‰ä¸€ç§å¯¹äºæœ‰weights éœ€è¦è°ƒèŠ‚çš„modelæ¥è¯´å¾ˆæœ‰ç”¨ï¼Œè€Œç¬¬äºŒç§ï¼Œå¯¹äºæ²¡æœ‰weightsè¦è°ƒèŠ‚çš„æ¨¡å‹ï¼ˆDecsion tree and Random Forestï¼‰æ¥è¯´æœ‰ç”¨ã€‚

### L1 and L2 regularization as penalties against model complexity

- L2 regularization:

<img src="https://render.githubusercontent.com/render/math?math=\| w \|_2^2 = \sum_{j=1}^{m}w_j^2
">

- L1 regularization:

<img src="https://render.githubusercontent.com/render/math?math=\| w \|_1 = \sum_{j=1}^{m}\vert w_j \vert
">


In contrast to L2 regularization, L1 regularization usually yields sparse feature vectors and most feature weights will be zero. 

å½“æˆ‘ä»¬çš„æ•°æ®æœ‰å¾ˆå¤šä¸ç›¸å…³çš„featureæ—¶(è¿™é‡Œçš„ä¸ç›¸å…³æ˜¯æŒ‡ä¸modelè¦é¢„æµ‹çš„ç»“æœä¸ç›¸å…³ï¼Œåº”è¯¥ä¸æ˜¯æŒ‡featureä¹‹é—´çš„ç›¸å…³æ€§)ï¼Œæˆ–è€…featureçš„æ•°ç›®å·²ç»å¤§äºsampleçš„æ•°ç›®æ—¶ï¼ŒL1 regularization å¯ä»¥ç”¨äºfeature selectionã€‚


### ä¸ºä»€ä¹ˆL2å¯ä»¥å‡è½»overfiting ï¼Œ L1 å¯ä»¥äº§ç”Ÿsparse features

ä¸‹é¢ç”¨å‡ ä½•æ–¹æ³•è¿›è¡Œè¡¨ç¤ºï¼Œå¯ä»¥æ¸…æ™°åœ°çœ‹å‡ºä¸¤è€…ä¹‹é—´çš„åŒºåˆ«ä»¥åŠä¸¤è€…æ˜¯å¦‚ä½•ä½œç”¨äºweightsç”¨äºå‡è½»overfittingã€‚

A geometric interpretation of L2 regularization

![L2 regularization](/assets/images/python_ml/L2.png)

ç”±äºL2çš„å›¾å½¢åœ¨åæ ‡ç³»ä¸­æ˜¯ä¸€ä¸ªåœ†ï¼Œè€Œcost functionè¿™é‡Œä½¿ç”¨çš„æ˜¯`sum of squared errors(SSE)`ã€‚è™½ç„¶å…¶ä»–çš„cost functionä¸ä¹‹ä¸åŒï¼Œä½†æ˜¯åŸºæœ¬æ€è·¯æ˜¯ä¸€è‡´çš„ã€‚

æˆ‘ä»¬çš„ç›®çš„æ˜¯åˆ°è¾¾cost functionä¸­é—´çš„Minimize costï¼ŒL2çš„ä½œç”¨åˆ™æ˜¯æŠŠweightsé™å®šåœ¨è¿™ä¸ªshadedåœˆä¸­ã€‚é‚£ä¹ˆè·ç¦»Minimize costæœ€è¿‘çš„åœˆä¸­çš„ç‚¹åˆ™æ˜¯ç›¸äº¤çš„è¾¹ç•Œç‚¹ã€‚æˆ‘ä»¬è¿™é‡Œçš„regularizationçš„å€¼å¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ªå®šå€¼ï¼Œå³L2çš„è¡¨è¾¾å€¼æ˜¯ä¸€ä¸ªå®šå€¼ã€‚å¦‚æœlambdaå˜å¤§ï¼Œé‚£ä¹ˆwå°±ä¼šå˜å°ï¼Œè¿™ä¸ªshaded åœˆä¹Ÿä¼šå˜å°ï¼Œè€Œweightsçš„å˜åŒ–èŒƒå›´ä¹Ÿä¼šå˜å°ã€‚å¦‚æœlambdaè¶‹äºæ— ç©·å¤§ï¼Œåˆ™wä¼šè¶‹äºé›¶ã€‚

 To summarize the main message of the example, our goal is to minimize the sum of the unpenalized cost plus the penalty term, which can be understood as adding bias and preferring a simpler model to reduce the variance in the absence of sufficient training data to fit the model.ç”±äºweightsçš„å˜åŒ–èŒƒå›´å˜çª„äº†(the intersection of shaded ball and cost function)ï¼Œå› æ­¤å¯ä»¥è®¤ä¸ºmodelå˜ç®€å•äº†ã€‚

A geometric interpretation of L1 regularization

![L1 regularization](/assets/images/python_ml/L1.png)

L1 çš„è¡¨è¾¾å¼æ˜¯ç»å¯¹å€¼ï¼Œå› æ­¤å®ƒçš„å›¾å½¢æ˜¯ä¸€ä¸ªçŸ©å½¢ï¼Œä¸­å¿ƒæ˜¯åŸç‚¹ã€‚å…¶ä¸cost functionçš„äº¤ç•Œå¤„å­˜åœ¨ä¸è½´ä¸Šï¼Œä¾‹å¦‚åœ¨å›¾ä¸­w1=0ã€‚å¦‚æœæ˜¯å¤šç»´çš„æ•°æ®ï¼Œé‚£ä¹ˆäº¤ç•Œåœ¨å…¶ä¸­ä¸€ç»´ä¸Šï¼Œé‚£ä¹ˆå…¶ä»–ç»´åˆ™ä¸º0ï¼Œå°±ä¼šå¯¼è‡´sparse featuresã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™ä¸ªç‰¹å¾æå–é‡è¦çš„featureã€‚è¿™é‡Œè®¤ä¸ºä¸ç­‰äº0çš„featureæ˜¯é‡è¦çš„featureã€‚

ä¸‹é¢ä½¿ç”¨sklearnæ¥æŸ¥çœ‹L1çš„æ•ˆæœï¼š

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(penalty='l1',
                        C=1.0, # Cå€¼æ˜¯lambdaçš„å€’æ•°ï¼ŒCå€¼è¶Šå°labmdaè¶Šå¤§ï¼Œæ›´å¤šçš„featureä¼šå˜ä¸º0ï¼Œfeatureè¶Šç¨€ç–ã€‚
                        solver='liblinear',
                        multi_class='ovr'
                       )

lr.fit(X_train_std,y_train)

lr.score(X_train_std,y_train)

lr.score(X_test_std,y_test)

lr.intercept_ 

lr.coef_ # weights çš„å€¼
```

>- lr.intercept_ : ç”±äºä½¿ç”¨çš„one vs rest(OvR) æ–¹æ³•ï¼Œå› æ­¤ä¼šæœ‰ä¸‰ä¸ªæˆªè·å³biasï¼Œthe first intercept belongs to the model that fits class 1 versus classes 2 and 3, the second value is the intercept of the model that fits class 2 versus classes 1 and 3, and the third value is the intercept of the model that fits class 3 versus classes 1 and 2

> - In scikit-learn, the intercept_ corresponds to ğ‘¤0(bias) and coef_ corresponds to the values ğ‘¤j(weights) for j > 0.

### Sequential feature selection algorithms

L1é€‚ç”¨äºæœ‰weightså¯ä»¥è°ƒèŠ‚çš„æ¨¡å‹ã€‚å¦‚æœæ²¡æœ‰weightså¯ä»¥è¿›è¡Œè°ƒèŠ‚æ¨¡å‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±éœ€è¦ dimensionality reductionã€‚

å‡å°‘ç»´åº¦æœ‰ä¸¤ç§æ–¹æ³•ï¼š

- feature selection ï¼šwe select a subset of the original featuresã€‚ä»åŸæœ‰çš„featuresä¸­é€‰æ‹©å‡ºæ¯”è¾ƒé‡è¦çš„featureã€‚
- feature extraction ï¼š derive information from the feature set to construct a new feature subspaceã€‚ä»åŸæœ‰çš„features spaceæ„å»ºæ–°çš„features subspaceï¼Œå³é€šè¿‡è½¬åŒ–æˆæ–°çš„featuresï¼Œå¹¶ä¸æ˜¯ç®€å•çš„æå–ã€‚

Sequential feature selection(SBS)å±äºç¬¬ä¸€ç§ï¼Œè¿™æ˜¯ä¸€ä¸ªgreedy algorithmã€‚ At each stage we eliminate the feature that causes the least performance loss after removal. 

### Assessing feature importance with random forests

ä½¿ç”¨sklearn è‡ªå¸¦çš„random forestæ¥è¯„ä¼°featureçš„é‡è¦æ€§ã€‚

```python
from sklearn.ensemble import RandomForestClassifier

feat_labels = df_wine.columns[1:]
forest = RandomForestClassifier(n_estimators=500,
                                random_state=1
                               )
forest.fit(X_train,y_train)

importances = forest.feature_importances_

indices = np.argsort(importances)[::-1]

for f in range(X_train.shape[1]):
    print("%2d) %-30s %f"% (f+1,
                           feat_labels[indices[f]],
                           importances[indices[f]]))

plt.title('Feature Importance')
plt.bar(range(X_train.shape[1]),
        importances[indices],
        align='center'
       )
plt.xticks(range(X_train.shape[1]),
           feat_labels[indices], rotation=90
          )
plt.xlim([-1, X_train.shape[1]])
plt.tight_layout()
plt.show()
```

å›¾å¯å‚è€ƒ[pythonçŸ¥è¯†ç‚¹](/docs/plot_graph#pltbar)


ä»ä¸Šé¢æˆ‘ä»¬å¯ä»¥æ ¹æ®importanceå¾—å‡ºfeaturesçš„é‡è¦æ€§ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡sklearnè‡ªå¸¦çš„[SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html?highlight=selectfrommodel#sklearn.feature_selection.SelectFromModel)æ¥é€‰æ‹©featuresã€‚

```python
from sklearn.feature_selection import SelectFromModel
sfm = SelectFromModel(estimator=forest, threshold=0.1, prefit=True) 
X_selected = sfm.transform(X_train)
```

> forest æ˜¯å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå› ä¸ºprefit=Trueã€‚
threshold=0.1,æ˜¯feature importanceçš„limitï¼ŒThe estimator should have a feature_importances_ or coef_ attribute after fitting.

# Chapter 5 : Compressing Data via Dimensionality Reduction

é€šè¿‡å‡å°‘æ•°æ®çš„ç»´åº¦æ¥æµ“ç¼©æ•°æ®å¯¹äºæ•°æ®çš„`å‚¨å­˜å’Œåˆ†æ`éå¸¸é‡è¦ã€‚
- å› ä¸ºå¯¹äºæ²¡æœ‰weightsè¿›è¡Œè°ƒèŠ‚çš„modelï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‡å°‘ç»´åº¦æ¥æå‡modelçš„performanceã€‚
- reducing the curse of dimensionality

 The difference between feature selection and feature extraction is that while we maintain the original features when we use feature selection algorithms, such as sequential backward selection, we use feature extraction to transform or project the data onto a new feature space.

 In the context of dimensionality reduction, feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information. åœ¨å‡å°‘ç»´åº¦çš„åŒæ—¶å°½å¯èƒ½ä¿ç•™ç›¸å…³çš„ä¿¡æ¯ã€‚

è¿™ä¸€ç« æ¶µç›–ä»¥ä¸‹topics about Dimensionality Reduction:
- Principal component analysis (PCA) for unsupervised data compression
- Linear discriminant analysis (LDA) as a supervised dimensionality reduction technique for maximizing class separability
- Nonlinear dimensionality reduction via kernel principal component analysis (KPCA)

## Principal component analysis (PCA)

PCAçš„ç›®çš„æ˜¯åœ¨åŸå§‹æ•°æ®ç©ºé—´ä¸­æ‰¾åˆ°äº’ç›¸å‚ç›´çš„ç»´åº¦ï¼Œåœ¨ä¸€ç»´åº¦ä¸‹ï¼Œæ•°æ®ä¹‹é—´çš„å·®å¼‚æœ€å¤§(variance),
æˆ‘ä»¬è®¤ä¸ºvarianceåŒ…å«äº†æ•°æ®çš„ä¿¡æ¯ï¼Œå·®å¼‚è¶Šå¤§åŒ…å«çš„ä¿¡æ¯è¶Šå¤šã€‚æ­¤æ—¶çš„ç»´åº¦ä¸åŸå§‹æ•°æ®çš„ç»´åº¦ä¸åŒï¼Œæ­¤æ—¶çš„ç»´åº¦å±äºåŸå§‹æ•°æ®ç©ºé—´çš„subspaceï¼Œå…¶ç»´åº¦æ•° <= åŸå§‹ç»´åº¦æ•°ã€‚

é‚£ä¹ˆå¦‚ä½•æ‰èƒ½æ‰¾åˆ°subspaceä¸­çš„ç»´åº¦å‘¢ï¼Ÿ`get the eigen pairs( eigenvectors,eigen values) from feature matrix `ï¼Œé€šè¿‡eigenvaluesçš„å¤§å°æŠŠeigen vectors  ç»„æˆä¸€ä¸ªmatrix Mï¼Œç„¶åé€šè¿‡xM=x_prime,æ±‚å¾—æ–°çš„xã€‚

å› ä¸ºå„ä¸ªç»´åº¦ä¹‹é—´çš„æ•°æ®è¦è¿›è¡Œæ¯”è¾ƒï¼Œè¦æœ‰å¯æ¯”æ€§ï¼Œå› æ­¤åœ¨æ±‚eigen pairsä¹‹å‰ï¼Œåº”è¯¥è¿›è¡Œstandardizationã€‚

æ­¥éª¤å¦‚ä¸‹ï¼š

1. Standardize the d-dimensional dataset.
2. Construct the covariance matrix.
3. Decompose the covariance matrix into its eigenvectors and eigenvalues.
4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.
5. Select k eigenvectors, which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (ğ‘˜ â‰¤ ğ‘‘).
6. Construct a projection matrix, W, from the "top" k eigenvectors.
7. Transform the d-dimensional input dataset, X, using the projection matrix, W, to obtain the new k-dimensional feature subspace.


sklearnä¸­æœ‰è‡ªå¸¦çš„functionå¯ä»¥æ±‚å¾—feature matrix çš„PCAï¼Œå…·ä½“çš„æ¯ä¸€æ­¥åˆ†æä¹¦æœ¬ä¸­æœ‰æ¶‰åŠï¼Œå†™çš„å¾ˆè¯¦ç»†ï¼Œè¿™é‡Œä¸èµ˜è¿°ã€‚

å…¶ä¸­å‡ ä¸ªå…³é”®ç‚¹æ¶‰åŠä¸€ä¸‹ï¼š
1. ç¬¬ä¸€ä¸»æˆåˆ†å«æœ‰çš„varianceæœ€å¤§ã€‚å³æœ€å¤§çš„eigenvalueå¯¹åº”çš„eigenvectorè½¬åŒ–åçš„featureã€‚
2. covariance(ç›¸å…³æ€§) between two features x_j and x_k:

<img src="https://render.githubusercontent.com/render/math?math=\sigma_{jk} = \frac{1}{n-1}\sum_{i=1}^{n}(x_j^{(i)}-\mu_j)(x_k^{(i)}-\mu_k)
">

> \mu_j and \mu_kæ˜¯ç›¸åº”åˆ—çš„å‡å€¼ã€‚å¦‚æœæˆ‘ä»¬æœ‰13åˆ—çš„featuresï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å¾—åˆ°13x13 covariance matrix sigmaã€‚

3. eigenvalue and eigenvector çš„å…³ç³»

<img src="https://render.githubusercontent.com/render/math?math=\sum\nu = \lambda\nu
">

> ç¬¬ä¸€ä¸ªæ˜¯covariance matrixï¼Œç¬¬äºŒä¸ªæ˜¯eigen vecotrï¼Œç¬¬ä¸‰ä¸ªæ˜¯eigenvalueã€‚

4. æˆ‘ä»¬å¾—åˆ°top k eigenvectorsåï¼Œç„¶åæŠŠè¿™äº›vectoråˆå¹¶æˆ`nxk`çš„matrix Wï¼Œç„¶åå†ç”¨XWè½¬åŒ–ä¸ºæ–°çš„featuresã€‚


### sklearn è‡ªå¸¦PCAæ–¹æ³•

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2) # åˆå§‹åŒ–ï¼Œè®¾å®šå‰©ä½™çš„ç»´åº¦æ•°
X_train_pca = pca.fit_transform(X_train_std) #è½¬åŒ–æ•°æ®X_train_std.æ³¨æ„è¿™é‡Œè½¬åŒ–çš„æ•°æ®æ˜¯standrization åçš„æ•°æ®
X_test_pca = pca.transform(X_test_std) # è½¬åŒ–X_test_std

```

å¦‚æœå‚æ•°`n_components=None`ï¼Œåˆ™æ‰€æœ‰çš„principle componentséƒ½ä¼šè¢«ä¿å­˜ï¼Œå¦‚æœè®¾å®šäº†å…·ä½“çš„æ•°å€¼ï¼Œå¦‚`n_components=2`,åˆ™åªä¼šä¿ç•™2ä¸ªä¸»æˆåˆ†ã€‚

```python
pca = PCA(n_components=None)
X_train_pca = pca.fit_transform(X_train_std)
pca.explained_variance_ratio_ # å„ä¸»æˆåˆ†çš„å æ¯”
pca.explained_variance_ # å„ä¸»æˆåˆ†çš„å¤§å°

```


## linear discriminant analysis(LDA)

LDA æ˜¯Supervised data compressionï¼Œå› æ­¤å®ƒä¼šç”¨åˆ°class labelçš„ä¿¡æ¯ã€‚

### PCA ä¸ LDAçš„åŒºåˆ«

- ä¸¤è€…éƒ½æ˜¯çº¿æ€§è½¬åŒ–( linear transformation techniques) æŠŠé«˜ç»´åº¦çš„æ•°æ®è½¬åŒ–æˆä¸ºä½ç»´åº¦çº¿æ€§æ•°æ®
- PCAæ˜¯unsupervisedï¼›LDAæ˜¯supervisedã€‚

### LDAçš„åˆ†ææµç¨‹

LDAå‡è®¾åˆ†æçš„æ•°æ®æ˜¯normally distributed.ä½†æ˜¯æœ‰æ—¶ä¸æ»¡è¶³ä¹Ÿå¯ä»¥ä½¿ç”¨è¿™ä¸ªLDAæŠ€æœ¯æ¥è¿›è¡Œé™ç»´ã€‚åˆ†æclass labelsæ˜¯å¦æ˜¯æ»¡è¶³normally distriubtedå¯ä»¥ä½¿ç”¨`np.bicount()`æ¥æŸ¥çœ‹æ¯ä¸ªlabelçš„distribution

let's briefly summarize the main steps that are required to perform LDA:

1. Standardize the d-dimensional dataset (d is the number of features).
2. For each class,compute the d-dimensional mean vector.
3. Construct the between-class scatter matrix,S_B, and the within-class scatter matrix,S_w.
4. Compute the eigenvectors and corresponding eigenvalues of the matrix,S_w^-1S_B.
5. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.
6. Choose the k eigenvectors that correspond to the k largest eigenvalues to construct a ğ‘‘ Ã— ğ‘˜-dimensional transformation matrix, W; the eigenvectors are the columns of this matrix.
7. Project the examples onto the new feature subspace using the transformation matrix, W.

ç”±ä»¥ä¸Šæˆ‘ä»¬å¯ä»¥çœ‹å‡ºä»ç¬¬5æ­¥å¼€å§‹LDA ä¸PCAæ–¹æ³•ç›¸åŒã€‚ä¹‹æ‰€ä»¥è¯´LDAæ˜¯supervised methodæ˜¯å› ä¸ºåœ¨ç¬¬2æ­¥ä½¿ç”¨åˆ°äº†class labelsçš„ä¿¡æ¯ã€‚

åœ¨LDAä¸­å¯ä»¥å¾—åˆ°çš„æœ€åé™ä½çš„ç»´åº¦æ•°ä¸º`c-1, cæ˜¯ classçš„ç§ç±»æ•°ç›®`ã€‚(In LDA, the number of linear discriminants is at most c-1, where c is the number of class labels)

### LDA via scikit-learn

LDAæ¯ä¸€æ­¥çš„è¯¦ç»†è¿‡ç¨‹åœ¨ä¹¦æœ¬ä¸­æœ‰ä»‹ç»ï¼Œæ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ã€‚ä½¿ç”¨sklearnè‡ªå¸¦çš„functionå¯ä»¥å®ŒæˆLDAã€‚å¦‚ä¸‹ï¼š

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=2) # n_components <= Using min(n_features, n_classes - 1) = min(13, 3 - 1) = 2 components.
X_train_lda = lda.fit_transform(X_train_std,y_train) # after standarization

lr = LogisticRegression(multi_class='ovr', random_state=1,
                       solver='lbfgs')

lr.fit(X_train_lda,y_train)
```

> - n_components è¦æ»¡è¶³min(n_features, n_classes - 1)
> - ldaè½¬åŒ–è¦å…ˆæŠŠæ•°æ®è¿›è¡Œstandarization


## kernel principal component analysis(KPCA) for nonlinear mappings

PCA ä¸ LDAéƒ½æ˜¯çº¿æ€§è½¬åŒ–ï¼Œè¦æ±‚æ•°æ®å¿…é¡»æ˜¯çº¿æ€§å¯åˆ†å‰²çš„ã€‚å¦‚æœæ•°æ®ä¸æ˜¯çº¿æ€§å¯åˆ†å‰²çš„ï¼Œé‚£ä¹ˆè¿™ä¸¤ç§æ–¹æ³•åˆ™æ²¡æœ‰äº†ç”¨æ­¦ä¹‹åœ°ã€‚è¿™æ—¶è¦ä½¿ç”¨kernel principal component analysis(KPCA).

KPCA å°±æ˜¯é€šè¿‡éçº¿æ€§è½¬åŒ–(kernel)æŠŠåŸå§‹æ•°æ®æŠ•å°„åˆ°æ›´é«˜çš„ç»´åº¦ï¼Œç„¶åå†ä½¿ç”¨å¸¸è§„çš„PCAæ–¹æ³•æŠŠç»´åº¦å†é™ä¸‹æ¥ã€‚è¿™æ—¶å°±å¯ä»¥ä½¿ç”¨linear classifierå¯¹é™ç»´åçš„æ•°æ®è¿›è¡Œè®­ç»ƒå’Œåˆ†æã€‚

é€šè¿‡ä¸€ç³»åˆ—å…¬å¼è½¬åŒ–åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºkernelå…¶å®å°±æ˜¯æ±‚ä¸¤ä¸ªvectors(samples in the database) çš„dot productã€‚å¯ä»¥çœ‹ä½œæ˜¯è¡¡é‡ä¸¤ä¸ªvectorçš„ç›¸ä¼¼æ€§ã€‚







