---
layout: default
title: Python machine learning:machine learning and deep learning with Python, scikit-learn, and TensorFlow 2
parent: books
date:   2021-11-10 20:28:05 -0400
categories: ml
nav_order : 1
---

---
{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
1. TOC
{:toc}
</details>

---

# Chapter 2 : Training Simple Machine Learning Algorithms for Classification

åœ¨è¿™ä¸€ç« ä¸­ä»‹ç»äº†æœºå™¨å­¦ä¹ ä¸­çš„ç¥ç»å…ƒçš„å‘å±•å†å²ã€‚åœ¨ä¸‹å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ç§ç¥ç»å…ƒã€‚

ç¬¬ä¸€ç§æ˜¯æ¨¡æ‹Ÿäººç±»ç¥ç»å…ƒç»†èƒçš„æ¿€æ´»æœºåˆ¶æœ€æ—©æå‡ºæ¥çš„ã€‚è¾“å…¥ç«¯æ¨¡æ‹Ÿäº†æ ‘çªï¼Œç„¶åæ•´åˆæ‰€æœ‰çš„ä¿¡å·ï¼Œä¸streshold è¿›è¡Œæ¯”è¾ƒï¼Œæ¥åˆ¤å®šclass label(1 or -1)ã€‚

ç¬¬äºŒç§æ”¹è¿›çš„åœ°æ–¹æ˜¯åœ¨net inputä¹‹åå¢åŠ äº†ä¸€ä¸ªactivation functionã€‚è¿™æ ·ä¸¤ç§ç¥ç»å…ƒåœ¨å¯¹weights è¿›è¡Œupdateæ—¶ï¼Œupdates çš„æ¥æºæ˜¯ä¸åŒçš„ã€‚ç¬¬ä¸€ç§ç›´æ¥æ¥æºäºoutputï¼Œç¬¬äºŒç§æ¥æºäºactivation functionã€‚å‰è€…æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œåè€…æ˜¯ä¸€ä¸ªfloat numberã€‚è¿™æ ·åè€…å¯ä»¥åˆ¤å®šåç§»æ­£å¸¸class label çš„ç¨‹åº¦ã€‚è€Œç¬¬ä¸€ç§ä¸å¯ä»¥ã€‚

![perceptron vs adaptive linear neuron](/assets/images/python_ml/perceptron.png)

åŒæ—¶è¿™ä¸€ç« è¿˜ä»‹ç»äº†ä¸ºä»€ä¹ˆä¼šæœ‰biasï¼Œåœ¨perceptronä¸­ä¸ºäº†ä½¿threshold=0.æˆ‘ä»¬è®¤ä¸ºæ·»åŠ äº†ä¸€ä¸ªbiasï¼Œå¹¶ä¸”biaså­˜æ”¾åœ¨weightsæ•°åˆ—çš„ç¬¬ä¸€ä¸ªã€‚

weightsåœ¨åˆå§‹åŒ–æ—¶ï¼Œå…¶åº”è¯¥æ˜¯å¾ˆå°çš„æ•°ï¼Œä½†ä¸èƒ½ä¸º0ï¼Œå¦‚æœä¸º0åˆ™ä¹¦æœ¬ä¸Šè¯´å…¶weightsçš„å˜åŒ–åªæ˜¯scalerçš„å˜åŒ–ï¼Œè€Œæ²¡æœ‰æ–¹å‘ä¸Šçš„å˜åŒ–ã€‚åŒæ—¶ä½¿ç”¨`w = np.random.norm(loc=0.0, scale=0.1, size = 1+ x.shape[1])`æ¥åˆå§‹åŒ–weightsã€‚

åœ¨å†™codeæ—¶åº”åŸ¹å…»è‰¯å¥½çš„documentã€‚åœ¨æœ¬ç« ä¸­å¯ä»¥å€Ÿé‰´ä¸€ä¸‹ï¼š

```md

â€œâ€â€œfunction or class definition,å…·ä½“æ˜¯å¹²ä»€ä¹ˆçš„

Parameters
------
p1 : type
    definition
p2 : type
    definition
    
Returns
-------
r1 : type
    definition
r2 : type

Examples
-------
give some simple examples

Other
-------
Description

â€œâ€â€œ

```


## weight update

`w += det_w`


![weights update](/assets/images/python_ml/perceptron_update.png)

ä»¥ä¸Šæ˜¯ç¬¬ä¸€ç§ç¥ç»å…ƒPerceptronçš„weightsè¿›è¡Œupdateæ—¶ï¼Œweightså˜åŒ–

![weights update](/assets/images/python_ml/ada_update.png)

ä»¥ä¸Šæ˜¯ç¬¬äºŒç§ç¥ç»å…ƒadaçš„weights è¿›è¡Œupdateæ—¶weightsçš„å˜åŒ–é‡ã€‚å…¶æ¥æºäºä»¥ä¸‹cost å‡½æ•°ã€‚åªéœ€è¦å¯¹wjè¿›è¡Œæ±‚åå¯¼å°±å¯ä»¥å¾—åˆ°ä»¥ä¸Šå…¬å¼ã€‚

![weights update](/assets/images/python_ml/ada_cost.png)

ä¸¤è€…çš„åŒºåˆ«å¦‚ä¸‹ï¼š
1. perceptron ï¼š æ¯ä¸€ä¸ªexample(row)å¯¹weightè¿›è¡Œæ›´æ–°ä¸€æ¬¡ï¼› adaï¼šå…¬å¼é‡Œé¢æœ‰æ±‚å’Œç¬¦å·ï¼Œå› æ­¤æ˜¯å¯¹æ‰€æœ‰çš„exampleéå†å®Œä¹‹åå†æ›´æ–°weightsã€‚åè€…å«åš`Batch Gradient descent`ã€‚
2. å¯¹äºperceptronï¼Œ`error = yi - y^i` å…¶ä¸­outputæ˜¯class labelï¼Œå³ 1 or -1ã€‚è€Œada error ä¸­çš„output æ˜¯ä¸€ä¸ªfloat numberã€‚
3. perceptronï¼šdeta_w ä¸­çš„xæ˜¯ä¸€ä¸ªrowä¸­çš„feturesã€‚adaï¼šæ˜¯ä¸€ä¸ªç‚¹ç§¯ã€‚

å¦‚æœæˆ‘ä»¬çš„æ•°æ®é‡å¾ˆå¤§ï¼Œé‚£ä¹ˆéå†å®Œæ‰€æœ‰çš„exampleså†æ›´æ–°weightsåˆ™ä¸å®é™…ã€‚å› æ­¤æˆ‘ä»¬ä¹Ÿå¯ä»¥æ¯ä¸€ä¸ªexample æ›´æ–°ä¸€æ¬¡weightsï¼Œè¿™å«åš`stochastic gradient descent`ã€‚

å¯¹äºlearning rate(eta)è¶…å‚æ•°ã€‚å¦‚æœå¤ªå¤§ï¼Œåˆ™ä¼šå‘ç”Ÿovershootï¼Œå³åœ¨minumnç‚¹æ¥å›çš„èµ°åŠ¨ã€‚å¦‚æœå¤ªå°ï¼Œåˆ™èšåˆå¤ªæ…¢ã€‚

å¯¹äºdataä¸­æ•°æ®æ•°é‡çº§ä¸ä¸€è‡´çš„æƒ…å†µï¼Œä¾‹å¦‚æœ‰çš„åˆ—ä¸º0.09ï¼Œæœ‰çš„åˆ—ä¸º19800ï¼Œé‚£ä¹ˆæ˜¾ç„¶ç›´æ¥ä½¿ç”¨ä¸¤è€…çš„æ•°æ®è¿›è¡Œparameter fitingã€‚åè€…å½±å“ä¼šå¾ˆå¤§ï¼Œè€Œå‰è€…å½±å“å°ã€‚ä¹Ÿä¼šå½±å“èšåˆé€Ÿåº¦ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œnormalizeã€‚å¸¸è§çš„normalizeä¸º `(data - mean)/std`ä½¿ç”¨numpy å¾ˆç®€å•ï¼š`(x-x.mean())/x.std()`


ä¸ºä»€ä¹ˆè¦æ·»åŠ activationï¼šè¿™æ˜¯å› ä¸ºactivationå¾—å‡ºçš„ç»“æœä¸true labelçš„errorå¯ä»¥è¡¡é‡ä¸ture label çš„è·ç¦»ç›¸å·®å¤šå°‘ã€‚è€Œä½¿ç”¨thresholdåçš„ç»“æœï¼Œåˆ™ä¸èƒ½å‡¸æ˜¾ä¸ä¹‹ç¨‹åº¦ã€‚åœ¨è¿™ä¸€ç« ä¸­ä½¿ç”¨çš„æ˜¯linear activationï¼Œå³ç›´æ¥è¾“å‡ºï¼Œæ²¡æœ‰å˜åŒ–ã€‚

ç¥ç»å…ƒçš„æ•°æ®æµåŠ¨è¿‡ç¨‹
1. net input ï¼š`net_input = np.dot(X,self.w_[1:]) + self.w_[0]`. self.w_[0] is the bias
2. activation function
3. compute the error : `error = true_labe - output`. here output came from activation. But if the perceptron nueron, then it is the final predicted label
4. get the predicted label by comparing the threshold
5. do the next example or next batch


# Chapter 3 : A Tour of Machine Learning Classifiers Using scikit-learn

The five main steps that are involved in training a supervised machine learning algorithm can be summarized as follows:
1. Selecting features and collecting labeled training examples.
2. Choosing a performance metric.
3. Choosing a classifier and optimization algorithm.
4. Evaluating the performance of the model.
5. Tuning the algorithm.

åœ¨è¿™ä¸€ç« ä¸­ä»‹ç»äº†sklearn ä¸­å‡ ä¸ªå¸¸ç”¨çš„åˆ†ç±»ç®—æ³•ï¼šlogic regressionï¼Œsvmï¼Œdecision tree, random forest, KNN(k nearest neighbours)ã€‚

è§£å†³çš„ç–‘æƒ‘æœ‰ï¼š
1. logic regresssion ï¼šsigmoid activation functionçš„ç”±æ¥ã€‚
2. å„ç®—æ³•çš„ä¼˜ç¼ºç‚¹ã€‚åé¢è¯¦ç»†è¯´æ˜
3. decision tree åˆ¤å®šnodeçš„è®¡ç®—è¿‡ç¨‹ï¼šé€šè¿‡IG(information gain)æ¥å†³å®šå¯¹å“ªä¸ªfeature è¿›è¡Œåˆ†å‰²
4. random forest æ˜¯åœ¨æ•´ä¸ªdataset ä¸­éšæœºæœ‰æ”¾å›çš„é€‰æ‹©ä¸€äº›examplesï¼Œç„¶åå†éšæœºé€‰æ‹©ä¸€äº›featuresæ¥build decision treeã€‚è€Œ sklearnä¸­é»˜è®¤æƒ…å†µä¸‹æ˜¯ç›´æ¥ä½¿ç”¨å…¨éƒ¨çš„datasetï¼Œç„¶åéšæœºé€‰æ‹©subfeatureæ¥è¿›è¡Œbuild treeç»„æˆrandom forestã€‚
5. overfitting and underfitting
6. regularization

## æ¦‚ç‡ï¼Œå‘ç”Ÿæ¯”(odds), å‡€è¾“å…¥(net_input) å’Œlogit å…¬å¼ä¹‹é—´çš„è”ç³»

é¦–å…ˆæ˜ç¡®å‡ ä¸ªæ„Ÿå¿µï¼Œå…·ä½“ç»†èŠ‚å¯å‚è€ƒ[csdn](https://blog.csdn.net/youyanyu3817/article/details/106279297)ï¼š
- æ¦‚ç‡ï¼šæ¦‚ç‡æ˜¯æŒ‡ä¸€ä¸ªäº‹ä»¶å‘ç”Ÿçš„å¯èƒ½æ€§
- å‘ç”Ÿæ¯”:å‘ç”Ÿæ¯”æŒ‡ä¸€ä»¶äº‹å‘ç”Ÿä¸å…¶ä¸å‘ç”Ÿçš„æ¦‚ç‡ä¹‹æ¯”ï¼Œ
- Logit :Logitæ˜¯ç»™å‘ç”Ÿæ¯”å–å¯¹æ•°ï¼Œå³log(Odds)ï¼Œå…¶ä¸­logæ˜¯è‡ªç„¶å¯¹æ•°ã€‚
- net_input = X\*w

å› ä¸ºæˆ‘ä»¬ä»ä¸Šä¸€ç« ä¸­äº†è§£åˆ°äº†ç¥ç»å…ƒä¸­çš„ä¿¡æ¯æµåŠ¨è¿‡ç¨‹ã€‚çŸ¥é“äº†net_inputã€‚åœ¨å¾—åˆ°net_inputä¹‹å‰çš„è¿‡ç¨‹åŸºæœ¬éƒ½æ˜¯ä¸€è‡´çš„ã€‚é‚£ä¹ˆåœ¨perceptronä¸­ç›´æ¥ä½¿ç”¨net_input ç»è¿‡thresholdçš„ç»“æœï¼ˆlabelï¼‰ä½œä¸ºerror çš„å‚æ•° Â·`error = true_labe - label`ï¼›è€Œåœ¨Ada ä¸­åˆ™æ˜¯ä½¿ç”¨linear activation functionã€‚å³ç›´æ¥ä½¿ç”¨net_inputæ¥è®¡ç®—error:`error = true_label - net_input`ã€‚

ç”±äºæœ€åä¸€ç§ç¥ç»å…ƒé‡‡å–çš„æ˜¯æ¦‚ç‡ä½œä¸ºè¯„åˆ¤ç»“æœã€‚å› æ­¤å¯¹äºäºŒåˆ†ç±»æ¨¡å‹æ¥è¯´ï¼Œç»“æœåœ¨[0,1]ä¹‹é—´ã€‚å› æ­¤æˆ‘ä»¬è¦ä½¿net_inputç»è¿‡activation function ä¹‹åç»“æœä¹Ÿåœ¨[0,1]ä¹‹é—´ã€‚

é‚£ä¹ˆä¸ºä»€ä¹ˆä¼šå¼•å…¥å‘ç”Ÿæ¯”å’Œlogitå‘¢ï¼Ÿæˆ‘æƒ³åˆ°çš„æ˜¯è¿™ä¸¤è€…ä¸æ¦‚ç‡æœ‰å…³ã€‚ç¬¬ä¸€ä½åƒèƒèŸ¹çš„äººå°è¯•äº†ä»¥ä¸‹æ–¹æ³•ï¼Œè§‰å¾—ä¸é”™ã€‚è®°ä½æˆ‘ä»¬çš„ç›®çš„æ˜¯è¦æ±‚æ¦‚ç‡ï¼Œä¸”ä¸net_inputæ‰¯ä¸Šå…³ç³»ã€‚

- å¯ä»¥è®© `p = net_input` å—ï¼Ÿ

ä¸å¯ä»¥ï¼Œå› ä¸ºè¿™æ ·çš„è¯net_inputçš„èŒƒå›´åªèƒ½æ§åˆ¶åœ¨[0,1]ï¼Œä¸æ–¹ä¾¿update weightsã€‚å¤ªçª„äº†ã€‚

- å¯ä»¥è®©`odds = net_input` å—ï¼Ÿ

ä¸å¯ä»¥ï¼Œå› ä¸ºå‘ç”Ÿæ¯”æœ‰ä¸å¯¹ç§°æ€§, å½“æ¦‚ç‡ä»0.1å¢åŠ åˆ°0.2ï¼Œæ—¶ï¼Œå‘ç”Ÿæ¯”å¢åŠ äº†0.139ï¼›å½“æ¦‚ç‡ä»0.8å¢åŠ åˆ°0.9æ—¶ï¼Œå‘ç”Ÿæ¯”å´å¢åŠ äº†5ï¼Œè™½ç„¶æ¦‚ç‡çš„å¢é‡ç›¸åŒï¼Œä½†å‘ç”Ÿæ¯”çš„å¢é‡å´å¤§å¤§çš„ä¸åŒã€‚å…¶ä»–ä¾‹å­å¯ä»¥å‚è€ƒ[csdn](https://blog.csdn.net/youyanyu3817/article/details/106279297) 

- å¯ä»¥è®©`logit = net_input` å—ï¼Ÿ

ä¸ºäº†è§£å†³oddsçš„ä¸å¯¹ç§°æ€§ï¼Œå¼•å…¥äº†logitã€‚å› ä¸ºlogå‡½æ•°å®¹æ˜“è®¡ç®—å’Œæ±‚å¯¼ã€‚å› æ­¤è®©`logit(p) = z`, here z = net_input = X\*w.

log(p/1-p) = z
- p/(1-p) = e^z
- p = e^z/(1+e^z)
- p = 1/(1+e^-z)

åˆ°è¿™é‡Œæˆ‘ä»¬æŠŠæ¦‚ç‡å’Œnet_inputè”ç³»åœ¨äº†ä¸€èµ·ï¼Œå¹¶ä¸”`1/(1+e^-z)`å°±æ˜¯activation functionã€‚

## é€šè¿‡activation function æ›´æ–°updates

![ada and logic regression difference](/assets/images/python_ml/ada_lr_nueron.png)

ä»å›¾ä¸­å¯ä»¥çœ‹å‡º ada ä¸ logic regression ä¹‹é—´çš„ä¸åŒä¹‹å¤„ï¼š
1. activation functionæ˜¯ä¸åŒçš„ã€‚ada æ˜¯linear activationï¼›logic regresionæ˜¯sigmoid activationã€‚
2. logic regression å¯ä»¥æ±‚å¾—å½“å‰class label çš„æ¦‚ç‡ã€‚

logic regression çš„cost functionæ˜¯æ±‚å¾—æœ€å¤§çš„likelihood. æ±‚deta_wçš„è¿‡ç¨‹å¦‚ä¸‹ï¼š
1. è·å¾—æœ€å¤§likelihoodï¼Œè¿™æ˜¯ä¸€ä¸ªä¹˜ç§¯å…¬å¼
2. æ±‚logè½¬åŒ–ä¸ºæ±‚å’Œå…¬å¼

ä¸ºä»€ä¹ˆè½¬åŒ–ä¸ºæ±‚å’Œå…¬å¼ï¼š
1. å› ä¸ºä¹˜ç§¯ä¼šå¯¼è‡´æ•°æ®æº¢å‡ºã€‚
2. æ±‚å¯¼æ–¹ä¾¿

## overfitting and underfitting

 If a model suffers from overfitting, we also say that the model has a high variance, 

Similarly, our model can also suffer from underfitting (high bias), which means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance
on unseen data.

overfitting : high variance
underfitting : high bias

## logic regression VS svm

logic regression æ›´å®¹æ˜“å—åˆ°outlier data çš„å½±å“ã€‚è€Œsvm å¯ä»¥é€šè¿‡è°ƒèŠ‚Cå€¼ æ¥æ”¹å˜svmçš„regularizationçš„ç¨‹åº¦ã€‚

## kernel SVM

å¦‚æœdata æ˜¯å¯ä»¥è¿›è¡Œçº¿æ€§åˆ’åˆ†çš„ï¼Œé‚£ä¹ˆä½¿ç”¨linear SVMå°±å¯ä»¥äº†ã€‚å¦‚æœä¸æ˜¯çº¿æ€§åˆ’åˆ†çš„(nonlinear sepratable),é‚£ä¹ˆå¯ä»¥ä½¿ç”¨kernel SVMè¿›è¡Œåˆ†å‰²ã€‚å…¶åŸºæœ¬æ€è·¯æ˜¯æŠŠåœ¨å½“å‰ç»´åº¦ä¸Šä¸å¯åˆ†çš„æ•°æ®è¿›è¡Œå‡ç»´ã€‚åœ¨é«˜ç»´ä¸Šå°±å¯ä»¥è½¬åŒ–æˆä¸ºçº¿æ€§å¯åˆ†äº†ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![kernel svm](/assets/images/python_ml/kernel_svm.png)



<img src="https://render.githubusercontent.com/render/math?math=IG(D_p,f) = I(D_p) - \sum_{j=1}^{m} \frac{N_j}{N_p} I(D_j)">

## regularization

The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter (weight) values. 

The most common form of regularization is so-called L2 regularization (sometimes also called L2 shrinkage or weight decay), which can be written as follows:

![r2 regularization](/assets/images/python_ml/r2_regularization.png)

![r2 regularization](/assets/images/python_ml/cost_lr.png)

1. ä¹‹æ‰€ä»¥å«åšweight decay æ˜¯å› ä¸ºåç§»é‡ä¸weight æœ‰å…³ã€‚
2. åŒæ—¶ä¸ºäº†åšregularizationä¹Ÿè¦è¿›è¡Œfeature normalizationï¼Œè¿™æ ·å„weightsä¹‹é—´æœ‰å¯æ¯”è¾ƒæ€§ã€‚


## Decision Tree

- å¦‚ä½•åˆ†å‰²æ¯ä¸ªnode

ä»¥ä¸‹å…¬å¼æ˜¯decision tree çš„ç›®æ ‡å‡½æ•°ã€‚å³æ¯æ¬¡åˆ†å‰²éƒ½ä¼šä½¿`IG(D,f)`æœ€å¤§ã€‚

<img src="https://render.githubusercontent.com/render/math?math=IG(D_p,f) = I(D_p) - \sum_{j=1}^{m} \frac{N_j}{N_p} I(D_j)">

Here, f is the feature to perform the split; `ğ·ğ‘` and `ğ·ğ‘—` are the dataset of the parent and jth child node; `I` is our impurity measure; `ğ‘ğ‘` is the total number of training examples at the parent node; and `Nj` is the number of example in the jth child node.

The information gain is simply the difference between the impurity of the parent node and the sum of the child node impuritiesâ€”the lower the impurities of the child nodes, the larger the information gain. 

ä¸ºäº†ç®€å•ï¼Œä¸€èˆ¬ä½¿ç”¨binary decision treeã€‚å› æ­¤ä»¥ä¸Šå…¬å¼ç®€åŒ–ä¸ºå¦‚ä¸‹å…¬å¼ã€‚åªæœ‰å·¦å³ä¸¤ä¸ªchildã€‚

<img src="https://render.githubusercontent.com/render/math?math=IG(D_p,f) = I(D_p) - \frac{N_{left}}{N_p} I(D_{left}) - \frac{N_{right}}{N_p} I(D_{right})">


### information gain in Decision Tree

åœ¨ä»¥ä¸Šå…¬å¼ä¸­çš„`I(D)`å³ä¸çº¯å‡€åº¦æ£€æµ‹(impurity measure)å¯ä»¥æœ‰ä»¥ä¸‹ä¸‰ç§æ–¹æ³•ï¼š

1. Gini impurity
2. Entropy
3. Classification error

- Entropy equation

<img src="https://render.githubusercontent.com/render/math?math=I_H(t) = - \sum_{i=1}^{c} p(i|t) \log_2p(i|t)
">

å‰ææ˜¯non-empty class `p(i|t) != 0`
Therefore, we can say that the entropy criterion attempts to maximize the mutual information in the tree.

- Gini impurity

The Gini impurity can be understood as a criterion to minimize the probability of misclassification:

<img src="https://render.githubusercontent.com/render/math?math=I_G(t) = \sum_{i=1}^{c} p(i|t)(1-p(i|t)) = 1 - \sum_{i=1}^{c}p(i|t)^2
">

Gini å¯ä»¥è®¤ä¸ºæ˜¯å°½é‡ä½¿è¯¯åˆ†ç±»çš„classæ¦‚ç‡æœ€å°ã€‚Gini ä¸Entropy åœ¨perfectly mixed çš„æƒ…å†µä¸‹ï¼Œå€¼æœ€å¤§ã€‚å¦‚æœä¸¤ä¸ªclassï¼Œæ¯ä¸ªclass çš„æ¦‚ç‡ä¸º0.5ï¼Œé‚£ä¹ˆGiniçš„å€¼æ˜¯`1-2*0.5^2=0.5`; entropy çš„ç»“æœæ˜¯1. ä½¿ç”¨Gini å’Œ Entropy çš„ç»“æœéƒ½å·®ä¸å¤šã€‚

- classification error

<img src="https://render.githubusercontent.com/render/math?math=I_E(t) = 1 - \max\{p(i|t)\}
">


classification error å¯ä»¥ç”¨äºå‰ªæï¼Œä½†æ˜¯å¹¶ä¸é€‚åˆç”¨äºæ ‘çš„å¢é•¿ã€‚å› ä¸ºå®ƒå¯¹æ¯ä¸ªnodesä¸­classæ¦‚ç‡çš„å˜åŒ–å¹¶ä¸æ•æ„Ÿã€‚è™½ç„¶probabilityæœ‰å˜åŒ–ï¼Œä½†æ˜¯å¾ˆå¯èƒ½ç»“æœä¸€è‡´ã€‚ä½†æ˜¯Giniå’ŒEntropyå´å¯ä»¥æŸ¥å‡ºè¿™äº›å˜åŒ–ï¼Œå¹¶ç»™å‡ºåˆç†çš„ç»“æœã€‚

ä»ä¸‹é¢çš„æ¡ˆä¾‹ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºä¸‰è€…çš„ä¸åŒä¹‹å¤„ï¼š

![classification error](/assets/images/python_ml/classification_error.png)

![Gini error](/assets/images/python_ml/gini_error.png)

![Entropy error](/assets/images/python_ml/entropy_error.png)


### å»ºç«‹Decision Tree

```python
from sklearn.tree import DecisionTreeClassifier
tree_model = DecisionTreeClassifier(criterion='gini',
                                   max_depth=4,
                                   random_state=1)
tree_model.fit(X_train,y_train)

from sklearn import tree
tree.plot_tree(tree_model)
plt.show()
```

> é€šè¿‡ä½¿ç”¨ `tree.plot_tree`æ¥æ˜¾ç¤ºtree çš„å†…éƒ¨æ¯ä¸ªnodeæ˜¯å¦‚ä½•åˆ’åˆ†çš„ã€‚

![decsion tree](/assets/images/python_ml/decsion_tree.png)

### Random Forest

The random forest algorithm can be summarized in four simple steps:

1. Draw a random bootstrap sample of size n (randomly choose n examples from the training dataset with replacement).
2. Grow a decision tree from the bootstrap sample. At each node:
a. Randomly select d features without replacement.
b. Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain.
3. Repeat the steps 1-2 k times.
4. Aggregate the prediction by each tree to assign the class label by majority vote. 

> In the step 2,  instead of evaluating all features to determine the best split at each node, we only consider a random subset of those.


- Advantage of Random Forest

1. we don't have to worry so much about choosing good hyperparameter values. We typically don't need to prune the random forest since the ensemble model is quite robust to noise from the individual decision trees.
2. The only parameter that we really need to care about in practice is the number of trees, k, (step 3) that we choose for the random forest.
3. Other parameters inclue the size, n, of the bootstrap sample (step 1), and the number of features, d, that are randomly chosen for each split (step 2.a), respectively.


### K-nearest neighbors â€“ a lazy learning algorithm

KNN is a lazy learning algorithm and don't have parameters.

KNN å¹¶ä¸é€‚ç”¨äºç»´åº¦å¾ˆé«˜çš„æ•°æ®ã€‚å› ä¸ºåœ¨é«˜ç»´åº¦ä¸ŠKNNçš„ç»“æœä¼šå˜å¾—å¾ˆç–æ¾ã€‚å› æ­¤å¦‚æœæ˜¯é«˜ç»´åº¦çš„æ•°æ®ï¼Œåº”è¯¥é¦–å…ˆè¿›è¡Œé™ç»´ã€‚

# chapter 4 : Building Good Training Datasets â€“ Data Preprocessing


## df.isna().sum()

ä½¿ç”¨df.isna().sum()æŸ¥çœ‹æ¯ä¸ªcolumnä¸­æœ‰å¤šå°‘missing valuesã€‚

## df.values

ä½¿ç”¨`df.values`å°†dataframeè½¬åŒ–ä¸ºnumpyã€‚sklearn ä¸­ä¸»è¦æ˜¯å¯¹numpyçš„æ•°æ®è¿›è¡Œå¤„ç†ã€‚ç°åœ¨sklearnä¸­å¾ˆå¤šfunctionéƒ½å¯ä»¥æ”¯æŒpandasã€‚æ‰€ä»¥å¦‚æœå¯ä»¥è½¬åŒ–ä¸ºnumpyåˆ™å°½é‡è½¬åŒ–ä¸ºnumpyåå†ä½¿ç”¨sklearnã€‚

## æ–¹æ³•ä¸€ï¼šç›´æ¥åˆ é™¤ [df.dorpna(axis=?)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)

æœ€ç®€å•ç²—æš´çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨`df.dropna`ç›´æ¥åˆ é™¤ã€‚

- å¦‚æœæƒ³åˆ é™¤å¸¦æœ‰NaNçš„rowsï¼š `df.dropna(axis=0)`
- åˆ é™¤å¸¦æœ‰NaNçš„columns ï¼š `df.dropna(axis=1)`
- only drop rows where all columns are NaN : `df.dropna(how='all')`
- drop rows that have fewer than 4 real values : `df.dropna(thresh=4)`
- only drop rows where NaN appear in specific columns (here: 'C'): `df.dropna(subset=['C'])`


## æ–¹æ³•äºŒï¼šæ’å€¼æ³• Imputing missing values

ç”±äºç›´æ¥åˆ é™¤æ•°æ®æœ‰ä»¥ä¸‹ç¼ºç‚¹ï¼š
- å¦‚æœåˆ é™¤å¤ªå¤šçš„æ•°æ®å¯¼è‡´æ•°é‡å¤ªå°‘ï¼Œéš¾äºåˆ†ææŒ–æ˜å‡ºæœ‰ç”¨çš„æ•°æ®ã€‚
- å¦‚æœåˆ é™¤äº†columnï¼Œè€Œæ­¤columnå¯èƒ½ä¸å¯¹classè¿›è¡Œåˆ†ç¦»å¾ˆé‡è¦ï¼Œå¯¼è‡´åˆ†ç±»ä¸å‡†ç¡®ã€‚

é‰´äºæ­¤ï¼Œæ›´å¸¸ç”¨çš„æ–¹æ³•æ˜¯ä½¿ç”¨æ’å€¼æ³•ã€‚å¸¸ç”¨çš„å·®å€¼æ–¹æ³•æ˜¯ä½¿ç”¨å‡å€¼(mean imputation)ã€‚

- ä½¿ç”¨sklearn ä¸­çš„`SimpleImputer`å¾ˆæ–¹ä¾¿å®Œæˆã€‚

```python
from sklearn.impute import SimpleImputer
import numpy as np
imr = SimpleImputer(missing_values=np.nan, strategy='mean')
imr = imr.fit(df.values)
imputed_data = imr.transform(df.values)
imputed_data
```

> [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) å¯¹numpyæ•°æ®è¿›è¡Œå¤„ç†ã€‚å› æ­¤ä½¿ç”¨`df.values`æŠŠdataframeè½¬åŒ–ä¸ºnumpy. strategyå‚æ•°é™¤äº†`mean`ä¹‹å¤–ï¼Œè¿˜æœ‰`median or most_frequent`ã€‚å½“æ•°æ®æ˜¯categorical feature æ—¶ä½¿ç”¨`most_frequent`æ˜¯éå¸¸æœ‰å¸®åŠ©çš„ã€‚


- ä½¿ç”¨[df.fillna(df.mean())](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) å¡«è¡¥ç¼ºå¤±å€¼


ä½¿ç”¨df.fillna(df.mean())æ¥å¡«è¡¥ç¼ºå¤±å€¼(å‡å€¼)

## Handle categorical missing values

ä»¥ä¸‹pythonä»£ç çš„æ¡ˆä¾‹å¦‚ä¸‹

```python
df = pd.DataFrame([
          ['green', 'M', 10.1, 'class2'],
           ['red', 'L', 13.5, 'class1'],
         ['blue', 'XL', 15.3, 'class2']])
df.columns = ['color','size','price','classlabel']
```

### ordinal and nominal features

- ordinal features : Ordinal features can be understood as categorical values that can be sorted or ordered. ä¾‹å¦‚è¡¬è¡«çš„size å¤§å°ï¼Œ XL > L > M.

- nominal features don't imply any order ï¼š è¡¬è¡«çš„é¢œè‰²æ˜¯nominal featureï¼Œæˆ‘ä»¬ä¸èƒ½è¯´red > greenã€‚

### ordinal features è½¬åŒ–

æˆ‘ä»¬å¯ä»¥è®¤ä¸ºè®¾å®šordinal features mappingæŠŠdata ä¸­çš„categorical featu è½¬åŒ–ä¸ºinteger valueã€‚

```python
size_mapping = {
    'XL':3,
    'L':2,
    'M':1
}
df['size'] = df['size'].map(size_mapping)
```

ä»¥ä¸Šçš„mappingæˆ‘ä»¬è®¤ä¸º`XL=L+1=M+2`è€Œå¾—å‡ºæ¥çš„ã€‚å¦‚æœæˆ‘ä»¬ä¸çŸ¥é“è¿™ç§relationshipï¼Œä½†æ˜¯æˆ‘ä»¬çŸ¥é“`XL>L>M`ã€‚æˆ‘ä»¬å¯ä»¥ç”¨å¦ä¸€ç§å½¢å¼çš„encodingã€‚æˆ‘ä»¬æŠŠ`>M or >L`çš„sizeè½¬åŒ–ä¸ºbinary valueã€‚

```python
df['x>M'] = df['size'].apply(lambda x:1 if x in {'L','XL'} else 0 )
df['x>L'] = df['size'].apply(lambda x:1 if x == 'XL' else 0)

```

### convert class label into intergers

å› ä¸ºsklearnå¯¹æ•°å­—å¤„ç†å¾ˆåœ¨è¡Œï¼Œå¯¹äºcategorical valueså¹¶ä¸å‹å¥½ã€‚å› æ­¤æˆ‘ä»¬æœ€å¥½æŠŠclass labelè½¬åŒ–ä¸ºintergeræ–¹ä¾¿machine learning modelè¿›è¡Œå¤„ç†ã€‚

- ä½¿ç”¨mapping

```python
class_mapping = {k:i for i,k in enumerate(np.unique(df['classlabel'].values))}

df['classlabel'].map(class_mapping)
```

- ä½¿ç”¨sklearnè‡ªå¸¦çš„LabelEncoder

```python
from sklearn.preprocessing import LabelEncoder
class_le = LabelEncoder()
y = class_le.fit_transform(df['classlabel'].values) # è½¬åŒ–ä¸ºinteger
class_le.inverse_transform(y) # è½¬åŒ–ä¸ºcategorical values
```

### one-hot encoding on nominal features

å¯¹äºnominal featuresï¼Œæˆ‘ä»¬ä¸èƒ½ä½¿ç”¨mapping æˆ–è€…LabelEncoderè½¬åŒ–æˆä¸ºintergerã€‚å› ä¸ºè¿™æ ·çš„è¯ï¼Œmachine learning model ä¼šè®¤ä¸ºå„integerä¹‹é—´å­˜åœ¨å¤§å°å…³ç³»ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦æŠŠæ¯ä¸ªnominal valueè½¬åŒ–ä¸ºä¸€ä¸ªdummy feature(binary value)ã€‚å³è¿™ä¸ªvalueæ˜¯å¦æ˜¯å­˜åœ¨çš„ã€‚

- æ–¹æ³•ä¸€ï¼šä½¿ç”¨sklearnè‡ªå¸¦çš„ [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=onehotencoder#sklearn.preprocessing.OneHotEncoder)

```python
from sklearn.preprocessing import OneHotEncoder
X = df.values

color_ohe = OneHotEncoder()

color_ohe.fit_transform(X[:,0].reshape(-1,1)).toarray()

```

> å› ä¸ºsklearnä¸­çš„function å¯¹numpyæ•°æ®è¿›è¡Œå¤„ç†ï¼Œå› æ­¤ç¬¬ä¸€æ­¥ä¾¿æ˜¯æŠŠdataframeçš„æ•°æ®è½¬åŒ–ä¸ºnumpyæ•°æ®(df.values)ã€‚`X[:,0].reshape(-1,1)` è¿™ä¸€æ­¥æ“ä½œæ˜¯æŠŠXä¸­çš„æŸä¸€åˆ—æå–å‡ºæ¥(æ­¤æ—¶æ˜¯rowçš„å½¢å¼)ï¼Œç„¶åå†è½¬åŒ–ä¸ºnX1çš„matrixå½¢å¼ï¼Œå› ä¸ºsklearnä¸­çš„function ä¸­çš„fit_transformåªå¯¹matrixè¿›è¡Œå¤„ç†ã€‚ä¸ºäº†æ˜¾ç¤ºç»“æœä½¿ç”¨`toarray()`

- æ–¹æ³•äºŒï¼šä½¿ç”¨sklearnè‡ªå¸¦çš„ [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html?highlight=columntransformer#sklearn.compose.ColumnTransformer)

```python
from sklearn.compose import ColumnTransformer

c_transf = ColumnTransformer([
    ('onehot',OneHotEncoder(),[0]),
    ('nothing','passthrough',[1,2])
])

c_transf.fit_transform(X).astype(float)
```

> The ColumnTransformer accepts a list of (name, transformer, column(s)) as above. æœ€åæˆ‘ä»¬ä½¿ç”¨astypeæŠŠnumpyçš„æ•°æ®è¿›è¡Œæ ¼å¼è½¬æ¢ï¼Œå°†objectæ ¼å¼è½¬åŒ–æˆä¸ºfloatæ ¼å¼ã€‚

- ä½¿ç”¨DataFrameè‡ªå¸¦çš„[get_dummies](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)method

```python
pd.get_dummies(df[['price', 'color', 'size']])
```

åœ¨ä¸Šé¢çš„è½¬åŒ–ä¸­æˆ‘ä»¬å¯¹æ¯ä¸€ä¸ªnominal vlaueéƒ½è¿›è¡Œäº†dummy binary valueè½¬åŒ–ã€‚è¿™æ ·å¯¼è‡´æˆ‘ä»¬è½¬åŒ–åçš„ç»“æœæ˜¯é«˜åº¦ç›¸å…³çš„ã€‚ä¾‹å¦‚å½“color_blue=0 and color_green=0ï¼Œé‚£ä¹ˆcolor_redä¸€å®šæ˜¯1.è¿™æ ·é«˜åº¦ç›¸å…³çš„matrixä¼šå¯¼è‡´matrixåœ¨è®¡ç®—æ—¶éå¸¸å›°éš¾(inverse)ã€‚å› æ­¤æˆ‘ä»¬è¦åœ¨dummy valuesä¸­åˆ é™¤ä¸€åˆ—ã€‚

```python
pd.get_dummies(df[['price','color','size']],drop_first=True)

color_ohe = OneHotEncoder(categories='auto', drop='first')
c_transf = ColumnTransformer([
    ('onehot',color_ohe,[0]),
    ('nothing','passthrough',[1,2])
])
c_transf.fit_transform(X).astype(float)
```

> åœ¨pd.get_dummiesä¸­æ·»åŠ å‚æ•°`drop_first=True`;åœ¨OneHotEncoderä¸­æ·»åŠ å‚æ•°`categories='auto', drop='first'`


## åˆ†å‰²æ•°æ®

ä½¿ç”¨sklearnè‡ªå¸¦çš„train_test_splitå¯¹æ•°æ®è¿›è¡Œåˆ†å‰²

```python
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y)
```

> åˆ†å‰²çš„æ¯”ä¾‹ä¸ºtrain:test = 7:3;Providing the class label array y as an argument to stratify ensures that both training and test datasets have the same class proportions as the original dataset.


## Bringing features onto the same scale

feature scaling æ˜¯éå¸¸é‡è¦çš„ä¸€æ­¥ã€‚é™¤äº† `Decision trees and random forests` è¿™ä¸¤ä¸ªalgorithmä¸éœ€è¦è¿›è¡Œfeature scalingä¹‹å¤–ï¼Œç»å¤§éƒ¨åˆ†machine learning algorithméœ€è¦feature scalingã€‚ä¾‹å¦‚ gradient descent optimization algorithm

`feature scaling çš„é‡è¦æ€§`: å¦‚æœä¸¤ä¸ªcolumn feature çš„scaleä¸åŒã€‚ä¸€ä¸ªæ˜¯1åˆ°10ï¼Œå¦ä¸€ä¸ªæ˜¯1 åˆ°1000.é‚£ä¹ˆåœ¨training çš„è¿‡ç¨‹ä¸­ï¼Œthe algorithmä¼šå¿™äºå¯¹scaleå¤§çš„é‚£ä¸ªfeatureçš„weights è¿›è¡Œupdateã€‚

feature scaling çš„ä¸¤ç§å½¢å¼ï¼š
- normalization:

å³ min-max scalingçš„å¦ä¸€ç§å½¢å¼ã€‚å¯ä»¥æŠŠæ•°æ®é™åˆ¶åœ¨[0,1]ä¹‹å†…ã€‚å› æ­¤å½“æˆ‘ä»¬éœ€è¦æŠŠæ•°æ®é™åˆ¶åœ¨æŸä¸€èŒƒå›´å†…æ—¶ï¼Œmin-max scaleæ˜¯éå¸¸æœ‰ç”¨çš„ã€‚

<img src="https://render.githubusercontent.com/render/math?math=x_{norm}^{(i)} = \frac{x^{(i)} - x_{min}}{x_{max} - x_{min}}
">

> Here, x^i is a particular example, x_min is the smallest value in the feature column, and x_max is the larget value

sklearn ä¸­çš„MinMaxScalerå¯¹æ•°æ®è¿›è¡Œmin-max scaling

```python
from sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)
```

- standardizationï¼š

<img src="https://render.githubusercontent.com/render/math?math=x_{std}^{(i)} = \frac{x^{(i)} - \mu_x}{\sigma_x}
">

> u_x is the sample mean of a particular feature colum, and \\sigma_x is the correspongding standard deviation.


standardization ä¸»è¦æ˜¯ç”¨äºmachine learning algorithmsï¼Œå°¤å…¶æ˜¯åœ¨optimization ç®—æ³•ä¸­ï¼Œä¾‹å¦‚ gradient descent.

- è¿™æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨weightsåˆå§‹åŒ–æ—¶å¾€å¾€æŠŠweightsåˆå§‹åŒ–ä¸ºéå¸¸æ¥è¿‘äº0çš„éšæœºæ•°ã€‚è¿™æ—¶æˆ‘ä»¬ä½¿ç”¨standardization æŠŠfeatureä¹Ÿè®¾ç½®æˆä¸weights parameteråŒæ ·çš„standard normal distribution(zero means and unit variance)ï¼Œè¿™æ ·`weightsä¼šå­¦ä¹ çš„æ›´å¿«`
- å¦ä¸€ä¸ªåŸå› æ˜¯standarization ä¿ç•™outliers çš„æœ‰ç”¨ä¿¡æ¯ï¼Œç›¸è¾ƒäºmin-max scalingï¼Œstandarizationå¯¹outliersä¸æ•æ„Ÿã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨sklearnä¸­çš„`StandardScaler` methodå¯¹features è¿›è¡Œstandardizationã€‚

```python
from sklearn.preprocessing import StandardScaler

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)
```

- [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)

å¦‚æœdatasetæ¯”è¾ƒå°ï¼Œä¸”æœ‰å¾ˆå¤šoutliersï¼Œåˆæˆ–è€…å‘ç”Ÿoverfittingï¼Œé‚£ä¹ˆRobustScaleræ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚

## Selecting meaningful features

å½“æˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡å‹æ—¶å‘ç”Ÿoverfittingï¼ŒåŸå› æ˜¯å½“å‰çš„æ¨¡å‹å¯¹ä¸å½“å‰çš„datasetæ¥è¯´å¤ªå¤æ‚äº†ï¼Œå¯¼è‡´æ¨¡å‹æœ‰å¾ˆé«˜çš„variance and has a high generalization error. 

Common solutions to reduce the generalization error are as follows(é˜»æ­¢overfittingçš„æ–¹æ³•):

- Collect more training data
- Introduce a penalty for complexity via regularization
- Choose a simpler model with fewer parameters
- Reduce the dimensionality of the data

ç¬¬ä¸€æ¡ï¼Œæ”¶é›†æ›´å¤šçš„æ•°æ®å¹¶ä¸å®ç”¨ã€‚
1. æˆ‘ä»¬ä¸çŸ¥é“æ˜¯å¦æ˜¯ç”±äºæ”¶é›†çš„feature nosie å¤ªå¤šå¯¼è‡´æ¨¡å‹å‘ç”Ÿhigh varianceï¼Œå› æ­¤æ”¶é›†æ›´å¤šçš„samplesä¹Ÿæ²¡æœ‰ç”¨ã€‚
2. æˆæœ¬å¤ªé«˜ï¼Œæˆ–è€…æ ¹æœ¬æ²¡æœ‰åŠæ³•æ”¶é›†æ›´å¤šçš„samplesã€‚

ä¸‹é¢æˆ‘ä»¬çœ‹ä¸€ä¸‹é€šè¿‡regularizationå’Œdimensionality reduction via feature selectionã€‚å‰ä¸€ç§å¯¹äºæœ‰weights éœ€è¦è°ƒèŠ‚çš„modelæ¥è¯´å¾ˆæœ‰ç”¨ï¼Œè€Œç¬¬äºŒç§ï¼Œå¯¹äºæ²¡æœ‰weightsè¦è°ƒèŠ‚çš„æ¨¡å‹ï¼ˆDecsion tree and Random Forestï¼‰æ¥è¯´æœ‰ç”¨ã€‚

### L1 and L2 regularization as penalties against model complexity

- L2 regularization:

<img src="https://render.githubusercontent.com/render/math?math=\| w \|_2^2 = \sum_{j=1}^{m}w_j^2
">

- L1 regularization:

<img src="https://render.githubusercontent.com/render/math?math=\| w \|_1 = \sum_{j=1}^{m}\vert w_j \vert
">


In contrast to L2 regularization, L1 regularization usually yields sparse feature vectors and most feature weights will be zero. 

å½“æˆ‘ä»¬çš„æ•°æ®æœ‰å¾ˆå¤šä¸ç›¸å…³çš„featureæ—¶(è¿™é‡Œçš„ä¸ç›¸å…³æ˜¯æŒ‡ä¸modelè¦é¢„æµ‹çš„ç»“æœä¸ç›¸å…³ï¼Œåº”è¯¥ä¸æ˜¯æŒ‡featureä¹‹é—´çš„ç›¸å…³æ€§)ï¼Œæˆ–è€…featureçš„æ•°ç›®å·²ç»å¤§äºsampleçš„æ•°ç›®æ—¶ï¼ŒL1 regularization å¯ä»¥ç”¨äºfeature selectionã€‚


### ä¸ºä»€ä¹ˆL2å¯ä»¥å‡è½»overfiting ï¼Œ L1 å¯ä»¥äº§ç”Ÿsparse features

ä¸‹é¢ç”¨å‡ ä½•æ–¹æ³•è¿›è¡Œè¡¨ç¤ºï¼Œå¯ä»¥æ¸…æ™°åœ°çœ‹å‡ºä¸¤è€…ä¹‹é—´çš„åŒºåˆ«ä»¥åŠä¸¤è€…æ˜¯å¦‚ä½•ä½œç”¨äºweightsç”¨äºå‡è½»overfittingã€‚

A geometric interpretation of L2 regularization

![L2 regularization](/assets/images/python_ml/L2.png)

ç”±äºL2çš„å›¾å½¢åœ¨åæ ‡ç³»ä¸­æ˜¯ä¸€ä¸ªåœ†ï¼Œè€Œcost functionè¿™é‡Œä½¿ç”¨çš„æ˜¯`sum of squared errors(SSE)`ã€‚è™½ç„¶å…¶ä»–çš„cost functionä¸ä¹‹ä¸åŒï¼Œä½†æ˜¯åŸºæœ¬æ€è·¯æ˜¯ä¸€è‡´çš„ã€‚

æˆ‘ä»¬çš„ç›®çš„æ˜¯åˆ°è¾¾cost functionä¸­é—´çš„Minimize costï¼ŒL2çš„ä½œç”¨åˆ™æ˜¯æŠŠweightsé™å®šåœ¨è¿™ä¸ªshadedåœˆä¸­ã€‚é‚£ä¹ˆè·ç¦»Minimize costæœ€è¿‘çš„åœˆä¸­çš„ç‚¹åˆ™æ˜¯ç›¸äº¤çš„è¾¹ç•Œç‚¹ã€‚æˆ‘ä»¬è¿™é‡Œçš„regularizationçš„å€¼å¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ªå®šå€¼ï¼Œå³L2çš„è¡¨è¾¾å€¼æ˜¯ä¸€ä¸ªå®šå€¼ã€‚å¦‚æœlambdaå˜å¤§ï¼Œé‚£ä¹ˆwå°±ä¼šå˜å°ï¼Œè¿™ä¸ªshaded åœˆä¹Ÿä¼šå˜å°ï¼Œè€Œweightsçš„å˜åŒ–èŒƒå›´ä¹Ÿä¼šå˜å°ã€‚å¦‚æœlambdaè¶‹äºæ— ç©·å¤§ï¼Œåˆ™wä¼šè¶‹äºé›¶ã€‚

 To summarize the main message of the example, our goal is to minimize the sum of the unpenalized cost plus the penalty term, which can be understood as adding bias and preferring a simpler model to reduce the variance in the absence of sufficient training data to fit the model.ç”±äºweightsçš„å˜åŒ–èŒƒå›´å˜çª„äº†(the intersection of shaded ball and cost function)ï¼Œå› æ­¤å¯ä»¥è®¤ä¸ºmodelå˜ç®€å•äº†ã€‚

A geometric interpretation of L1 regularization

![L1 regularization](/assets/images/python_ml/L1.png)

L1 çš„è¡¨è¾¾å¼æ˜¯ç»å¯¹å€¼ï¼Œå› æ­¤å®ƒçš„å›¾å½¢æ˜¯ä¸€ä¸ªçŸ©å½¢ï¼Œä¸­å¿ƒæ˜¯åŸç‚¹ã€‚å…¶ä¸cost functionçš„äº¤ç•Œå¤„å­˜åœ¨ä¸è½´ä¸Šï¼Œä¾‹å¦‚åœ¨å›¾ä¸­w1=0ã€‚å¦‚æœæ˜¯å¤šç»´çš„æ•°æ®ï¼Œé‚£ä¹ˆäº¤ç•Œåœ¨å…¶ä¸­ä¸€ç»´ä¸Šï¼Œé‚£ä¹ˆå…¶ä»–ç»´åˆ™ä¸º0ï¼Œå°±ä¼šå¯¼è‡´sparse featuresã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™ä¸ªç‰¹å¾æå–é‡è¦çš„featureã€‚è¿™é‡Œè®¤ä¸ºä¸ç­‰äº0çš„featureæ˜¯é‡è¦çš„featureã€‚

ä¸‹é¢ä½¿ç”¨sklearnæ¥æŸ¥çœ‹L1çš„æ•ˆæœï¼š

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(penalty='l1',
                        C=1.0, # Cå€¼æ˜¯lambdaçš„å€’æ•°ï¼ŒCå€¼è¶Šå°labmdaè¶Šå¤§ï¼Œæ›´å¤šçš„featureä¼šå˜ä¸º0ï¼Œfeatureè¶Šç¨€ç–ã€‚
                        solver='liblinear',
                        multi_class='ovr'
                       )

lr.fit(X_train_std,y_train)

lr.score(X_train_std,y_train)

lr.score(X_test_std,y_test)

lr.intercept_ 

lr.coef_ # weights çš„å€¼
```

>- lr.intercept_ : ç”±äºä½¿ç”¨çš„one vs rest(OvR) æ–¹æ³•ï¼Œå› æ­¤ä¼šæœ‰ä¸‰ä¸ªæˆªè·å³biasï¼Œthe first intercept belongs to the model that fits class 1 versus classes 2 and 3, the second value is the intercept of the model that fits class 2 versus classes 1 and 3, and the third value is the intercept of the model that fits class 3 versus classes 1 and 2

> - In scikit-learn, the intercept_ corresponds to ğ‘¤0(bias) and coef_ corresponds to the values ğ‘¤j(weights) for j > 0.

### Sequential feature selection algorithms

L1é€‚ç”¨äºæœ‰weightså¯ä»¥è°ƒèŠ‚çš„æ¨¡å‹ã€‚å¦‚æœæ²¡æœ‰weightså¯ä»¥è¿›è¡Œè°ƒèŠ‚æ¨¡å‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±éœ€è¦ dimensionality reductionã€‚

å‡å°‘ç»´åº¦æœ‰ä¸¤ç§æ–¹æ³•ï¼š

- feature selection ï¼šwe select a subset of the original featuresã€‚ä»åŸæœ‰çš„featuresä¸­é€‰æ‹©å‡ºæ¯”è¾ƒé‡è¦çš„featureã€‚
- feature extraction ï¼š derive information from the feature set to construct a new feature subspaceã€‚ä»åŸæœ‰çš„features spaceæ„å»ºæ–°çš„features subspaceï¼Œå³é€šè¿‡è½¬åŒ–æˆæ–°çš„featuresï¼Œå¹¶ä¸æ˜¯ç®€å•çš„æå–ã€‚

Sequential feature selection(SBS)å±äºç¬¬ä¸€ç§ï¼Œè¿™æ˜¯ä¸€ä¸ªgreedy algorithmã€‚ At each stage we eliminate the feature that causes the least performance loss after removal. 

### Assessing feature importance with random forests

ä½¿ç”¨sklearn è‡ªå¸¦çš„random forestæ¥è¯„ä¼°featureçš„é‡è¦æ€§ã€‚

```python
from sklearn.ensemble import RandomForestClassifier

feat_labels = df_wine.columns[1:]
forest = RandomForestClassifier(n_estimators=500,
                                random_state=1
                               )
forest.fit(X_train,y_train)

importances = forest.feature_importances_

indices = np.argsort(importances)[::-1]

for f in range(X_train.shape[1]):
    print("%2d) %-30s %f"% (f+1,
                           feat_labels[indices[f]],
                           importances[indices[f]]))

plt.title('Feature Importance')
plt.bar(range(X_train.shape[1]),
        importances[indices],
        align='center'
       )
plt.xticks(range(X_train.shape[1]),
           feat_labels[indices], rotation=90
          )
plt.xlim([-1, X_train.shape[1]])
plt.tight_layout()
plt.show()
```

å›¾å¯å‚è€ƒ[pythonçŸ¥è¯†ç‚¹](/docs/plot_graph#pltbar)


ä»ä¸Šé¢æˆ‘ä»¬å¯ä»¥æ ¹æ®importanceå¾—å‡ºfeaturesçš„é‡è¦æ€§ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡sklearnè‡ªå¸¦çš„[SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html?highlight=selectfrommodel#sklearn.feature_selection.SelectFromModel)æ¥é€‰æ‹©featuresã€‚

```python
from sklearn.feature_selection import SelectFromModel
sfm = SelectFromModel(estimator=forest, threshold=0.1, prefit=True) 
X_selected = sfm.transform(X_train)
```

> forest æ˜¯å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå› ä¸ºprefit=Trueã€‚
threshold=0.1,æ˜¯feature importanceçš„limitï¼ŒThe estimator should have a feature_importances_ or coef_ attribute after fitting.

# Chapter 5 : Compressing Data via Dimensionality Reduction

é€šè¿‡å‡å°‘æ•°æ®çš„ç»´åº¦æ¥æµ“ç¼©æ•°æ®å¯¹äºæ•°æ®çš„`å‚¨å­˜å’Œåˆ†æ`éå¸¸é‡è¦ã€‚
- å› ä¸ºå¯¹äºæ²¡æœ‰weightsè¿›è¡Œè°ƒèŠ‚çš„modelï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‡å°‘ç»´åº¦æ¥æå‡modelçš„performanceã€‚
- reducing the curse of dimensionality

 The difference between feature selection and feature extraction is that while we maintain the original features when we use feature selection algorithms, such as sequential backward selection, we use feature extraction to transform or project the data onto a new feature space.

 In the context of dimensionality reduction, feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information. åœ¨å‡å°‘ç»´åº¦çš„åŒæ—¶å°½å¯èƒ½ä¿ç•™ç›¸å…³çš„ä¿¡æ¯ã€‚

è¿™ä¸€ç« æ¶µç›–ä»¥ä¸‹topics about Dimensionality Reduction:
- Principal component analysis (PCA) for unsupervised data compression
- Linear discriminant analysis (LDA) as a supervised dimensionality reduction technique for maximizing class separability
- Nonlinear dimensionality reduction via kernel principal component analysis (KPCA)

## Principal component analysis (PCA)

PCAçš„ç›®çš„æ˜¯åœ¨åŸå§‹æ•°æ®ç©ºé—´ä¸­æ‰¾åˆ°äº’ç›¸å‚ç›´çš„ç»´åº¦ï¼Œåœ¨ä¸€ç»´åº¦ä¸‹ï¼Œæ•°æ®ä¹‹é—´çš„å·®å¼‚æœ€å¤§(variance),
æˆ‘ä»¬è®¤ä¸ºvarianceåŒ…å«äº†æ•°æ®çš„ä¿¡æ¯ï¼Œå·®å¼‚è¶Šå¤§åŒ…å«çš„ä¿¡æ¯è¶Šå¤šã€‚æ­¤æ—¶çš„ç»´åº¦ä¸åŸå§‹æ•°æ®çš„ç»´åº¦ä¸åŒï¼Œæ­¤æ—¶çš„ç»´åº¦å±äºåŸå§‹æ•°æ®ç©ºé—´çš„subspaceï¼Œå…¶ç»´åº¦æ•° <= åŸå§‹ç»´åº¦æ•°ã€‚

é‚£ä¹ˆå¦‚ä½•æ‰èƒ½æ‰¾åˆ°subspaceä¸­çš„ç»´åº¦å‘¢ï¼Ÿ`get the eigen pairs( eigenvectors,eigen values) from feature matrix `ï¼Œé€šè¿‡eigenvaluesçš„å¤§å°æŠŠeigen vectors  ç»„æˆä¸€ä¸ªmatrix Mï¼Œç„¶åé€šè¿‡xM=x_prime,æ±‚å¾—æ–°çš„xã€‚

å› ä¸ºå„ä¸ªç»´åº¦ä¹‹é—´çš„æ•°æ®è¦è¿›è¡Œæ¯”è¾ƒï¼Œè¦æœ‰å¯æ¯”æ€§ï¼Œå› æ­¤åœ¨æ±‚eigen pairsä¹‹å‰ï¼Œåº”è¯¥è¿›è¡Œstandardizationã€‚

æ­¥éª¤å¦‚ä¸‹ï¼š

1. Standardize the d-dimensional dataset.
2. Construct the covariance matrix.
3. Decompose the covariance matrix into its eigenvectors and eigenvalues.
4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.
5. Select k eigenvectors, which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (ğ‘˜ â‰¤ ğ‘‘).
6. Construct a projection matrix, W, from the "top" k eigenvectors.
7. Transform the d-dimensional input dataset, X, using the projection matrix, W, to obtain the new k-dimensional feature subspace.


sklearnä¸­æœ‰è‡ªå¸¦çš„functionå¯ä»¥æ±‚å¾—feature matrix çš„PCAï¼Œå…·ä½“çš„æ¯ä¸€æ­¥åˆ†æä¹¦æœ¬ä¸­æœ‰æ¶‰åŠï¼Œå†™çš„å¾ˆè¯¦ç»†ï¼Œè¿™é‡Œä¸èµ˜è¿°ã€‚

å…¶ä¸­å‡ ä¸ªå…³é”®ç‚¹æ¶‰åŠä¸€ä¸‹ï¼š
1. ç¬¬ä¸€ä¸»æˆåˆ†å«æœ‰çš„varianceæœ€å¤§ã€‚å³æœ€å¤§çš„eigenvalueå¯¹åº”çš„eigenvectorè½¬åŒ–åçš„featureã€‚
2. covariance(ç›¸å…³æ€§) between two features x_j and x_k:

<img src="https://render.githubusercontent.com/render/math?math=\sigma_{jk} = \frac{1}{n-1}\sum_{i=1}^{n}(x_j^{(i)}-\mu_j)(x_k^{(i)}-\mu_k)
">

> \mu_j and \mu_kæ˜¯ç›¸åº”åˆ—çš„å‡å€¼ã€‚å¦‚æœæˆ‘ä»¬æœ‰13åˆ—çš„featuresï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å¾—åˆ°13x13 covariance matrix sigmaã€‚

3. eigenvalue and eigenvector çš„å…³ç³»

<img src="https://render.githubusercontent.com/render/math?math=\sum\nu = \lambda\nu
">

> ç¬¬ä¸€ä¸ªæ˜¯covariance matrixï¼Œç¬¬äºŒä¸ªæ˜¯eigen vecotrï¼Œç¬¬ä¸‰ä¸ªæ˜¯eigenvalueã€‚

4. æˆ‘ä»¬å¾—åˆ°top k eigenvectorsåï¼Œç„¶åæŠŠè¿™äº›vectoråˆå¹¶æˆ`nxk`çš„matrix Wï¼Œç„¶åå†ç”¨XWè½¬åŒ–ä¸ºæ–°çš„featuresã€‚


### sklearn è‡ªå¸¦PCAæ–¹æ³•

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2) # åˆå§‹åŒ–ï¼Œè®¾å®šå‰©ä½™çš„ç»´åº¦æ•°
X_train_pca = pca.fit_transform(X_train_std) #è½¬åŒ–æ•°æ®X_train_std.æ³¨æ„è¿™é‡Œè½¬åŒ–çš„æ•°æ®æ˜¯standrization åçš„æ•°æ®
X_test_pca = pca.transform(X_test_std) # è½¬åŒ–X_test_std

```

å¦‚æœå‚æ•°`n_components=None`ï¼Œåˆ™æ‰€æœ‰çš„principle componentséƒ½ä¼šè¢«ä¿å­˜ï¼Œå¦‚æœè®¾å®šäº†å…·ä½“çš„æ•°å€¼ï¼Œå¦‚`n_components=2`,åˆ™åªä¼šä¿ç•™2ä¸ªä¸»æˆåˆ†ã€‚

```python
pca = PCA(n_components=None)
X_train_pca = pca.fit_transform(X_train_std)
pca.explained_variance_ratio_ # å„ä¸»æˆåˆ†çš„å æ¯”
pca.explained_variance_ # å„ä¸»æˆåˆ†çš„å¤§å°

```


## linear discriminant analysis(LDA)

LDA æ˜¯Supervised data compressionï¼Œå› æ­¤å®ƒä¼šç”¨åˆ°class labelçš„ä¿¡æ¯ã€‚

### PCA ä¸ LDAçš„åŒºåˆ«

- ä¸¤è€…éƒ½æ˜¯çº¿æ€§è½¬åŒ–( linear transformation techniques) æŠŠé«˜ç»´åº¦çš„æ•°æ®è½¬åŒ–æˆä¸ºä½ç»´åº¦çº¿æ€§æ•°æ®
- PCAæ˜¯unsupervisedï¼›LDAæ˜¯supervisedã€‚

### LDAçš„åˆ†ææµç¨‹

LDAå‡è®¾åˆ†æçš„æ•°æ®æ˜¯normally distributed.ä½†æ˜¯æœ‰æ—¶ä¸æ»¡è¶³ä¹Ÿå¯ä»¥ä½¿ç”¨è¿™ä¸ªLDAæŠ€æœ¯æ¥è¿›è¡Œé™ç»´ã€‚åˆ†æclass labelsæ˜¯å¦æ˜¯æ»¡è¶³normally distriubtedå¯ä»¥ä½¿ç”¨`np.bicount()`æ¥æŸ¥çœ‹æ¯ä¸ªlabelçš„distribution

let's briefly summarize the main steps that are required to perform LDA:

1. Standardize the d-dimensional dataset (d is the number of features).
2. For each class,compute the d-dimensional mean vector.
3. Construct the between-class scatter matrix,S_B, and the within-class scatter matrix,S_w.
4. Compute the eigenvectors and corresponding eigenvalues of the matrix,S_w^-1S_B.
5. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.
6. Choose the k eigenvectors that correspond to the k largest eigenvalues to construct a ğ‘‘ Ã— ğ‘˜-dimensional transformation matrix, W; the eigenvectors are the columns of this matrix.
7. Project the examples onto the new feature subspace using the transformation matrix, W.

ç”±ä»¥ä¸Šæˆ‘ä»¬å¯ä»¥çœ‹å‡ºä»ç¬¬5æ­¥å¼€å§‹LDA ä¸PCAæ–¹æ³•ç›¸åŒã€‚ä¹‹æ‰€ä»¥è¯´LDAæ˜¯supervised methodæ˜¯å› ä¸ºåœ¨ç¬¬2æ­¥ä½¿ç”¨åˆ°äº†class labelsçš„ä¿¡æ¯ã€‚

åœ¨LDAä¸­å¯ä»¥å¾—åˆ°çš„æœ€åé™ä½çš„ç»´åº¦æ•°ä¸º`c-1, cæ˜¯ classçš„ç§ç±»æ•°ç›®`ã€‚(In LDA, the number of linear discriminants is at most c-1, where c is the number of class labels)

### LDA via scikit-learn

LDAæ¯ä¸€æ­¥çš„è¯¦ç»†è¿‡ç¨‹åœ¨ä¹¦æœ¬ä¸­æœ‰ä»‹ç»ï¼Œæ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ã€‚ä½¿ç”¨sklearnè‡ªå¸¦çš„functionå¯ä»¥å®ŒæˆLDAã€‚å¦‚ä¸‹ï¼š

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=2) # n_components <= Using min(n_features, n_classes - 1) = min(13, 3 - 1) = 2 components.
X_train_lda = lda.fit_transform(X_train_std,y_train) # after standarization

lr = LogisticRegression(multi_class='ovr', random_state=1,
                       solver='lbfgs')

lr.fit(X_train_lda,y_train)
```

> - n_components è¦æ»¡è¶³min(n_features, n_classes - 1)
> - ldaè½¬åŒ–è¦å…ˆæŠŠæ•°æ®è¿›è¡Œstandarization


## kernel principal component analysis(KPCA) for nonlinear mappings

PCA ä¸ LDAéƒ½æ˜¯çº¿æ€§è½¬åŒ–ï¼Œè¦æ±‚æ•°æ®å¿…é¡»æ˜¯çº¿æ€§å¯åˆ†å‰²çš„ã€‚å¦‚æœæ•°æ®ä¸æ˜¯çº¿æ€§å¯åˆ†å‰²çš„ï¼Œé‚£ä¹ˆè¿™ä¸¤ç§æ–¹æ³•åˆ™æ²¡æœ‰äº†ç”¨æ­¦ä¹‹åœ°ã€‚è¿™æ—¶è¦ä½¿ç”¨kernel principal component analysis(KPCA).

KPCA å°±æ˜¯é€šè¿‡éçº¿æ€§è½¬åŒ–(kernel)æŠŠåŸå§‹æ•°æ®æŠ•å°„åˆ°æ›´é«˜çš„ç»´åº¦ï¼Œç„¶åå†ä½¿ç”¨å¸¸è§„çš„PCAæ–¹æ³•æŠŠç»´åº¦å†é™ä¸‹æ¥ã€‚è¿™æ—¶å°±å¯ä»¥ä½¿ç”¨linear classifierå¯¹é™ç»´åçš„æ•°æ®è¿›è¡Œè®­ç»ƒå’Œåˆ†æã€‚

é€šè¿‡ä¸€ç³»åˆ—å…¬å¼è½¬åŒ–åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºkernelå…¶å®å°±æ˜¯æ±‚ä¸¤ä¸ªvectors(samples in the database) çš„dot productã€‚å¯ä»¥çœ‹ä½œæ˜¯è¡¡é‡ä¸¤ä¸ªvectorçš„ç›¸ä¼¼æ€§ã€‚

ä¹¦ä¸­æœ‰å¯¹KPCAçš„æ¯ä¸€æ­¥çš„pythonå®æ–½è¿‡ç¨‹ï¼Œè¿™é‡Œä¸èµ˜è¿°ã€‚ä¸‹é¢ä½¿ç”¨sklearnè‡ªå¸¦çš„æ–¹æ³•ã€‚

```python
from sklearn.datasets import make_moons
from sklearn.decomposition import KernelPCA

X,y = make_moons(n_samples=100,random_state=123)

scikit_kpca = KernelPCA(n_components=2,
                       kernel='rbf',gamma=15)
X_skernpca = scikit_kpca.fit_transform(X)

plt.scatter(X_skernpca[y==0,0],X_skernpca[y==0,1],c='red',marker='^',alpha=0.5)
plt.scatter(X_skernpca[y==1,0],X_skernpca[y==1,1],c='blue',marker='o',alpha=0.5)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.tight_layout()
plt.show()
```

# Chapter 6:Learning Best Practices for Model Evaluation and Hyperparameter Tuning

è¿™ä¸€ç« ä¸»è¦æ¶‰åŠçš„æ˜¯æ¨¡å‹æ•ˆæœçš„è¯„ä¼°æ–¹æ³•å’Œæ ‡å‡†ï¼Œä»¥åŠå¦‚ä½•è¿›è¡Œè°ƒå‚æ•°ã€‚

## piplines

åœ¨sklearnä¸­æœ‰ç°æˆçš„functionå¯ä»¥ä½¿ç”¨ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline

pipe_lr = make_pipeline(StandardScaler(),
                       PCA(n_components=2),
                        LogisticRegression(random_state=1,
                                          solver='lbfgs')
                       )
pipe_lr.fit(X_train,y_train)

y_pred = pipe_lr.predict(X_test)

pipe_lr.score(X_test,y_test)
```

åœ¨ä»¥ä¸Šä»£ç ä¸­æˆ‘ä»¬ä½¿ç”¨sklearn.pipline ä¸­çš„function make_piplineæ¥ç”Ÿæˆpiplineã€‚

The make_pipeline function takes an arbitrary number of scikit-learn `transformers` (objects that support the fit and transform methods as input), followed by a scikit- learn `estimator` that implements the fit and predict methods. 

pipline ä¸­å¯ä»¥åŒ…å«ä»»æ„ä¸ªtransformerï¼Œè¿™ä¸ªtransformerè¦æœ‰fit å’Œ transform functionã€‚piplineçš„æœ€åæ˜¯estimatorï¼Œå…¶è¦æœ‰fit å’Œ predict functionã€‚


If we call the fit method of Pipeline, the data will be passed down a series of transformers via fit and transform calls on these intermediate steps until it arrives at the estimator object (the final element
in a pipeline). The estimator will then be fitted to the transformed training data

If we feed a dataset to the predict call of a Pipeline object instance, the data will pass through the intermediate steps via transform calls. In the final step, the estimator object will then return a prediction on the transformed data.

å¦‚æœæˆ‘ä»¬ä½¿ç”¨pipline.fité‚£ä¹ˆpiplineä¸­çš„transformerå°±ä¼šè¿è¡Œfit_transform çš„methodå’Œestimatorçš„fit methodã€‚å¦‚æœä½¿ç”¨pipline.predictï¼Œåˆ™piplineä¸­çš„transformerå°±ä¼šè¿è¡Œtransform methodï¼Œestimator è¿è¡Œpredict methodã€‚

![process of pipline](/assets/images/python_ml/06_pipline.png)

## Using k-fold cross-validation to assess model performance

æˆ‘ä»¬éœ€è¦æŠŠæ•°æ®è¿›è¡Œåˆ†å‰²ï¼Œä¾‹å¦‚åˆ†æˆtrainï¼Œtest datasetã€‚è¿™æ ·çš„é—®é¢˜æ˜¯å¦‚æœæˆ‘ä»¬éœ€è¦ä½¿ç”¨ train dataset æ¥è®­ç»ƒæ¨¡å‹ï¼Œ test dataset æ¥è¯„ä¼°ä¸åŒçš„æ¨¡å‹çš„å¥½åã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨test dataset ä¸€æ¬¡æ¬¡çš„ç”¨äºæ¨¡å‹çš„é€‰æ‹©ï¼Œé‚£ä¹ˆä¹Ÿæ˜¯å˜ç›¸çš„ä½¿ç”¨test dataset æ¥è®­ç»ƒæ¨¡å‹ã€‚

 However, if we reuse the same test dataset over and over again during model selection, it will become part of our training data and thus the model will be more likely to overfit.

`æ›´å¥½çš„æ–¹æ³•æ˜¯æŠŠdataset åˆ†å‰²æˆï¼štrain,validation, test dataset.`

A better way of using the holdout method for model selection is to separate the data into three parts: a training dataset, a validation dataset, and a test dataset.

![dataset split](/assets/images/python_ml/06_split.png)


## K-fold cross-validation

å¦ä¸€ç§æ›´å¥½çš„åˆ†å‰²æ•°æ®å¹¶è¿›è¡Œè®­ç»ƒå’Œé€‰æ‹©ï¼Œæœ€åå†è¿›è¡ŒéªŒè¯çš„æ–¹æ³•æ˜¯k-fold cross-validation.

é‡‡å–ä¸æ”¾å›çš„æŠ½æ ·æ–¹æ³•ï¼ŒæŠŠtraining dataset åˆ†å‰²æˆä¸ºkä»½ã€‚ç„¶åä½¿ç”¨k-1ä»½çš„æ•°æ®ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œå‰©ä¸‹çš„ä¸€ä»½ç”¨äºmodel selection è¯„ä¼°æ¨¡å‹çš„performanceã€‚è¿™ç§è¿‡ç¨‹é‡å¤kæ¬¡ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥å¾—åˆ°kä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹å’Œperformance estimatesã€‚ç„¶åå†æ±‚å‡å€¼å°±å¯ä»¥å¾—åˆ°è¿™ä¸ªæ¨¡å‹çš„å¥½åã€‚

é€šè¿‡performance estimates çš„å‡å€¼ï¼Œæˆ‘ä»¬é€‰æ‹©å‡ºäº†modelå¹¶å¾—åˆ°äº†è¿™ä¸ªæ¨¡å‹çš„æœ€ä¼˜å‚æ•°ã€‚ä¸‹ä¸€æ­¥ï¼Œå†ä½¿ç”¨æ•´ä¸ªtraining dataset æ¥è®­ç»ƒé€‰æ‹©å‡ºæ¥çš„æ¨¡å‹ã€‚
æœ€åä½¿ç”¨test datasetæ¥å¯¹æ¨¡å‹åšæœ€åçš„è¯„ä¼°ã€‚

Since k-fold cross-validation is a resampling technique without replacement,
the advantage of this approach is that each example will be used for training
and validation (as part of a test fold) exactly once, which yields a lower-variance estimate of the model performance than the holdout method.

![k fold cross validation](/assets/images/python_ml/06_k_fold.png)

ä¸€èˆ¬kå€¼é€‰æ‹©10ã€‚å¦‚æœæ˜¯éå¸¸å°‘çš„æ•°æ®ï¼Œé‚£ä¹ˆkå€¼ä¼šå¢å¤§ã€‚å¦‚æœæ•°æ®å¾ˆå¤šï¼Œkä¹Ÿå¯ä»¥é€‰æ‹©5ã€‚

kå€¼å¢å¤§ï¼Œé‚£ä¹ˆmodel trainingçš„æ•°æ®çš„ç›¸ä¼¼æ€§ä¼šå˜å¤§ï¼Œå› æ­¤å¯¼è‡´æ¨¡å‹çš„varianceä¼šå˜å¤§(å‘ç”Ÿ overfitting)ã€‚

å¯¹äºk-fold cross vallidationæ–¹æ³•çš„æ”¹è¿›æ˜¯stratified k-fold cross-validation, which can yield better bias and variance estimates, especially in cases of unequal class proportionsã€‚

In stratified cross- validation, the class label proportions are preserved in each fold to ensure that each fold is representative of the class proportions in the training datasetã€‚

ä½¿ç”¨sklearn è‡ªå¸¦çš„[Stratified KFold function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html).

```python
import numpy as np
from sklearn.model_selection import StratifiedKFold

kfold = StratifiedKFold(n_splits=10).split(X_train,y_train)

scores = []
for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(X_train[train],y_train[train])
    score = pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)
    print('Flod: %2d, Class dist.: %s, Acc: %.3f'%(k+1,
        np.bincount(y_train[train]), score))
```
> StratifiedKFold(n_splits=10).split æŠŠæ•°æ®åˆ†å‰²æˆ10ä»½ï¼Œ è¿”å›çš„æ˜¯indincesã€‚åœ¨ä½¿ç”¨splitåˆ†å‰²æ—¶æ˜¯æ ¹æ®yå€¼è¿›è¡Œstratifyã€‚

é™¤äº†ä¸Šè¿°ä¸€ç§æ–¹æ³•ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç›´æ¥ä½¿ç”¨[cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html?highlight=cross_val_score#sklearn.model_selection.cross_val_score) ç›´æ¥è¯„ä¼°model estimatesã€‚

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(estimator=pipe_lr,
                        X=X_train,
                        y= y_train,
                        cv=10,
                        n_jobs=1)
```

> ä½¿ç”¨cross_val_score çš„æ–¹æ³•å¥½å¤„æ—¶å¯ä»¥å¤šçº¿ç¨‹å¤„ç†ã€‚å½“n_jobs=-1ï¼Œä½¿ç”¨å…¨éƒ¨çš„CUPåŒæ—¶è¿›è¡Œå¤„ç†ã€‚

## Debugging algorithms with learning and validation curves

æ¨¡å‹å¸¸è§çš„ä¸¤ç§é—®é¢˜æ—¶high bias and high variance.å¦‚ä¸‹æ‰€ç¤º

![model issues](/assets/images/python_ml/06_model_issues.png)

å¦‚æœæ—¶underfitting,é‡‡å–çš„ç­–ç•¥æ˜¯ï¼š
- å¢åŠ æ¨¡å‹çš„å‚æ•°ï¼Œè®©æ¨¡å‹å˜å¾—å¤æ‚ã€‚
- æ”¶é›†æ›´å¤šçš„featuresï¼Œæˆ–è€…æ„å»ºæ–°çš„features
- é€šè¿‡é™ä½regularizationçš„ç¨‹åº¦ï¼Œå¦‚SVMæˆ–è€…logic regression

å¦‚æœæ˜¯overfittingï¼Œé‡‡å–çš„ç­–ç•¥æ˜¯ï¼š
- æ”¶é›†æ›´å¤šçš„æ•°æ®samples
- å‡å°‘æ¨¡å‹çš„å¤æ‚åº¦
- å¢åŠ regularization çš„ç¨‹åº¦
- å‡å°‘feature çš„ç»´åº¦ï¼Œé™ç»´


å¦‚æœå¢åŠ training samples ç”¨äºè®­ç»ƒåˆ™å¯èƒ½ä¼šå¯¼è‡´overfittingï¼Œå› æ­¤å¤šå°‘çš„train samples æ¯”è¾ƒåˆé€‚å‘¢ï¼Ÿå¯ä»¥ä½¿ç”¨skealrn ä¸­çš„[learning_curve function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html?highlight=learning_curve#sklearn.model_selection.learning_curve).

```python
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
pipe_lr = make_pipeline(StandardScaler(),
                       LogisticRegression(penalty='l2',
                                         random_state=1,
                                         solver='lbfgs',
                                         max_iter=10000))

train_sizes,train_scores, test_scores = \
learning_curve(estimator=pipe_lr,
              X=X_train,
              y= y_train,
              train_sizes=np.linspace(0.1,1.0,10),
              cv=10, # å°†xfengeæˆå‡ åˆ†ï¼Œå¦‚æœæ˜¯æ•´æ•°åˆ™æ˜¯æŒ‰ç…§stratified k-foldæ–¹æ³•è¿›è¡Œåˆ†å‰²
              n_jobs=1) # n_jobs=-1 ä½¿ç”¨å…¨éƒ¨çš„CPUè¿›è¡Œè®¡ç®—

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores,axis=1)

test_mean = np.mean(test_scores,axis=1)
test_std = np.std(test_scores, axis=1)

plt.plot(train_sizes,train_mean,color='blue',
        marker='o',
        markersize=5, label='Training accuracy')
plt.fill_between(train_sizes,
                train_mean+train_std,
                train_mean-train_std,
                alpha=0.15,
                color='blue')
plt.plot(train_sizes,test_mean,color='green', linestyle='--',
        marker='s',markersize=5,
        label='Validation accuracy')
plt.fill_between(train_sizes,
                 test_mean+test_std,
                 test_mean-test_std,
                 alpha = 0.15,
                 color='green'
                )
plt.grid()
plt.xlabel('Number of training examples')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.ylim([0.8,1.03])
plt.show()
```

> learning_curve : Determines cross-validated training and test scores for different training set sizes.æ ¹æ®training sizeçš„ä¸åŒè¿”å›train score and test score.

> np.linspace çš„ç”¨æ³•å‚è€ƒ[python åªæ˜¯ç‚¹](/docs/python.html#numpylinspace)

> plt.fill_between : æ˜¯åœ¨train_mean+train_std,train_mean-train_stdä¹‹é—´å¡«å……ï¼Œxè½´ä¸ºtrain_sizes.è¯¦ç»†å†…å®¹å¯å‚è€ƒ[matplotlib](/docs/plot_graph.html#pltfill_between)

> [plt.grid()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.grid.html) : åœ¨å›¾ä¸­ç”»å‡ºæ ¼å­çº¿

![accuracy curve](/assets/images/python_ml/06_accuracy_curve.png)

### Addressing over- and underfitting with validation curves

validation curve ä¸learning curve å¾ˆç›¸ä¼¼ã€‚ä¸åŒä¹‹å¤„æ˜¯learning curveæ˜¯æ ¹æ®train sizeçš„å¤§å°æ±‚å¾—train score å’Œtest scoreã€‚è€Œvalidation curve åˆ™æ˜¯å¯ä»¥äººä¸ºåœ°è°ƒæ§æ¨¡å‹å†…çš„å‚æ•°ï¼Œè¿™æ ·å°±å¯ä»¥åœ¨æ¨¡å‹å‘ç”Ÿover - and underfittingæ—¶å¯¹æ¨¡å‹è¿›è¡Œè°ƒæ§ï¼Œçœ‹å“ªä¸€ä¸ªå‚æ•°æ›´åˆé€‚ã€‚

åœ¨sklearnä¸­ä½¿ç”¨[validation_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html)

```python
from sklearn.model_selection import validation_curve
param_range = [0.001,0.01, 0.1, 1.0, 10.0, 100.0]
# param_range = [10, 100,1000]
train_scores, test_scores = validation_curve(estimator=pipe_lr,
                                            X=X_train,
                                            y= y_train,
                                            param_name='logisticregression__C',
                                            param_range=param_range,
                                            cv=10)

train_mean= np.mean(train_scores,axis=1)
train_std = np.std(train_scores,axis=1)
test_mean = np.mean(test_scores,axis=1)
test_std = np.std(test_scores, axis=1)

plt.plot(param_range,train_mean,
        color='blue',marker='o',
        markersize=5,label='Training accuracy')
plt.fill_between(param_range,train_mean+train_std,
                train_mean-train_std,
                alpha=0.15,
                color='blue')
plt.plot(param_range,test_mean,
        color='green',linestyle='--',
        marker='s',markersize=5,
        label='Validation accuracy')
plt.fill_between(param_range,test_mean+test_std,
                test_mean-test_std,alpha=0.15,
                color='green')
plt.grid()
plt.xscale('log')
plt.legend(loc='lower right')
plt.xlabel('Parameter C')
plt.ylabel('Accuracy')
plt.ylim([0.8,1.0])
plt.show()
```

> æœ‰ä»¥ä¸Šä»£ç å¯ä»¥çœ‹å‡ºvalidation curveéœ€è¦ç¡®å®šå¯¹å“ªä¸ªparameterè¿›è¡Œä¼˜åŒ–(param_name)ï¼Œä»¥åŠä¼˜åŒ–çš„èŒƒå›´æ˜¯å¤šå°‘(param_range)ã€‚

> è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯param_nameä¸­å¼•ç”¨çš„å‚æ•°æ˜¯piplineä¸­LogisticRegressionä¸­çš„å‚æ•°ï¼Œç”±äºåœ¨piplineä¸­ä¼šæŠŠåç§°éƒ½å˜ä¸ºå°å†™ï¼Œå› æ­¤ä½¿ç”¨`logisticregression__C`ï¼Œ`C`ä¸ºLRä¸­çš„å‚æ•°ï¼Œä¸­é—´çš„è¿æ¥ä½¿ç”¨åŒä¸‹åˆ’çº¿ã€‚

### Fine-tuning machine learning models via grid search

Learning curve æ˜¯å¯¹train sizesè¿›è¡Œä¼˜åŒ–ï¼Œvalidation curveæ˜¯å¯¹æŸä¸€å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚å¦‚æœç›¸å¯¹å¤šä¸ªå‚æ•°è¿›è¡Œä¼˜åŒ–ï¼Œé‚£ä¹ˆæˆ‘ä»¬åˆ™éœ€è¦grid searchã€‚

åœ¨æœºå™¨å­¦ä¹ ä¸­æˆ‘ä»¬æœ‰ä¸¤ç±»å‚æ•°éœ€è¦ä¼˜åŒ–ï¼š
- those that are learned
from the training data, for example, the weights in logistic regressionã€‚éœ€è¦é€šè¿‡training dataè®­ç»ƒæ¨¡å‹çš„weightsã€‚
- the parameters of a learning algorithm that are optimized separatelyã€‚the regularization parameter in logistic regression or the depth parameter of a decision tree. æ¨¡å‹æœ¬èº«éœ€è¦è®¤ä¸ºè®¾å®šçš„å‚æ•°ï¼Œä¾‹å¦‚L1 æˆ–è€…L2 regularization çš„ç¨‹åº¦ï¼ŒDecision tree çš„æ·±åº¦ï¼Œrandom forest ä¸­æ ‘çš„æ•°ç›®ã€‚

The grid search approach is quite simple: it's a brute-force exhaustive search paradigm where we specify a list of values for different hyperparameters, and the computer evaluates the model performance for each combination to obtain the optimal combination of values from this setã€‚

ç›´æ¥ä½¿ç”¨sklearn ä¸­çš„[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV)

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

pipe_svc = make_pipeline(StandardScaler(),
                        SVC(random_state=1))

param_range = [0.0001, 0.001, 0.01, 0.1,
              1.0, 10.0, 100.0, 1000.0]

param_grid = [
    {
        'svc__C':param_range, # ä½¿ç”¨piplineä¸­çš„å¼•ç”¨è§„åˆ™
        'svc__kernel':['linear']
    },
    {
        'svc__C':param_range,
        'svc__gamma':param_range,
        'svc__kernel':['rbf']
    }
]

gs = GridSearchCV(estimator=pipe_svc,
                 param_grid=param_grid,
                 scoring='accuracy',
                 cv=10,
                 refit=True,
                 n_jobs=-1)


gs = gs.fit(X_train,y_train)

print(gs.best_score_)

print(gs.best_params_)

clf = gs.best_estimator_
```

> gs.best_score_ : è·å¾—æ¨¡å‹æœ€é«˜çš„å€¼ï¼› gs.best_params_ : è·å¾—æ¨¡å‹æœ€é«˜çš„å€¼çš„å‚æ•° ; gs.best_estimator_ : è·å¾—æœ€å¥½çš„æ¨¡å‹.

ç”±äºæˆ‘ä»¬åœ¨GridSearchCVä¸­è®¾å®šäº†`refit=True`,é‚£ä¹ˆæ¨¡å‹åœ¨æ‰¾åˆ°æœ€é«˜çš„scoreçš„å‚æ•°æ—¶ï¼Œä¼šä½¿ç”¨å…¨éƒ¨çš„X_trainï¼Œy_trainå†è®­ç»ƒè¿™ä¸ªæ¨¡å‹ä¸€æ¬¡ã€‚ç›¸å½“äºå¦‚ä¸‹ä»£ç ï¼š

```python
clf = gs.best_estimator_ # å¾—åˆ°å«æœ‰æœ€ä½³å‚æ•°çš„æ¨¡å‹ï¼Œå¦‚æœrefit=Trueï¼Œé‚£ä¹ˆä¸‹é¢é‚£ä¸€æ­¥å…¶å®æ—¶æ²¡æœ‰å¿…è¦çš„ã€‚
 
clf.fit(X_train,y_train)
```

Grid search æ˜¯å¯¹å…¨éƒ¨å¯èƒ½æ€§çš„å‚æ•°ç»„åˆè¿›è¡Œå°è¯•ï¼Œè¿™ä¼šå¸¦æ¥å¾ˆå¤§çš„è¿ç®—é‡ã€‚å¦ä¸€ç§å¯æ›¿ä»£çš„æ–¹æ³•æ˜¯[RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)ã€‚è¿™ç§æ–¹æ³•åªæ˜¯ä»å‚æ•°ä¸­sample ä¸€äº›å‚æ•°è¿›è¡Œéšæœºç»„åˆï¼Œç„¶ååˆ¤å®šæ¨¡å‹çš„ä¼˜åŠ£ã€‚è¿™ç§æ–¹æ³•é€‚ç”¨äºå‚æ•°æ˜¯è¿ç»­æ€§åˆ†å¸ƒä¸”éå¸¸å¤šçš„å‚æ•°çš„æƒ…å†µã€‚

## nested cross-validation

å¦‚æœæˆ‘ä»¬æ—¢æƒ³è°ƒèŠ‚æ¨¡å‹çš„å‚æ•°ï¼ŒåŒæ—¶ä¹Ÿæƒ³åœ¨ä¸åŒçš„æ¨¡å‹ä¸­é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æŠŠ`GridSearchCV`å’Œ`cross_val_score`ç»“åˆåœ¨ä¸€èµ·ã€‚å…¶åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š
1. å¤–å±‚å¾ªç¯æ˜¯æŠŠæ•°æ®è¿›è¡Œk-fold cross-validation,æŠŠæ•°æ®åˆ†å‰²æˆtrain_set and test setã€‚
2. å†…éƒ¨è¿˜æœ‰ä¸€ä¸ªå¾ªç¯é‚£å°±æ˜¯å¯¹å½“æ¨¡å‹è¿›è¡Œè°ƒèŠ‚å‚æ•°ã€‚æ­¤æ—¶ä¸€èˆ¬æ˜¯æŠŠtrain data setåˆ†å‰²æˆä¸¤ä»½ï¼Œä¸€ä»½ç”¨äºtrain modelï¼Œä¸€ä»½ç”¨äºvalidation modelã€‚ç„¶åæ‰¾åˆ°åœ¨å½“å‰æ•°æ®ä¸‹æœ€ä¼˜å‚æ•°çš„æ¨¡å‹ã€‚
3. ä½¿ç”¨ä¸Šä¸€æ­¥ä¸­çš„æœ€ä¼˜å‚æ•°æ¨¡å‹å¯¹test setè¿›è¡Œè¯„ä¼°åˆ†æ•°ï¼Œå¾—åˆ°kä¸ªscoreã€‚ä¸€èˆ¬ç¬¬ä¸€æ­¥æ˜¯k=5,ç¬¬ä¸‰æ­¥cv=2ï¼›

In nested cross-validation, we have an outer k-fold cross-validation loop to split
the data into training and test folds, and an inner loop is used to select the model using k-fold cross-validation on the training fold. After model selection, the test fold is then used to evaluate the model performance. The following figure explains the concept of nested cross-validation with only five outer and two inner folds, which can be useful for large datasets where computational performance is important; this particular type of nested cross-validation is also known as `5x2 cross-validation`:

![nested cross-validation](/assets/images/python_ml/06_nest_cross.png)


```python
gs = GridSearchCV(estimator=pipe_svc,
                 param_grid=param_grid,
                 scoring='accuracy', # ä½¿ç”¨accuracyä½œä¸ºé€‰æ‹©æ¨¡å‹çš„ä¾æ®
                 cv=2) # å†…å±‚å¾ªç¯æŠŠæ•°æ®åˆ†å‰²æˆ2ä»½ï¼Œç„¶åé€‰æ‹©å‡ºæœ€ä¼˜å‚æ•°çš„æ¨¡å‹

scores = cross_val_score(gs,X_train,y_train,
                        scoring='accuracy',cv=5) # è¿™æ˜¯å¤–å±‚å¾ªç¯ï¼ŒæŠŠoriginal dataset åˆ†å‰²æˆä¸º5ä»½ã€‚å¯¹test set æ±‚å€¼æ—¶ä½¿ç”¨accuracyã€‚

np.mean(scores),np.std(scores)
# output : (0.9736263736263737, 0.014906219743132467)

from sklearn.tree import DecisionTreeClassifier
gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),
                 param_grid=[{'max_depth':[1,2,3,4,5,6,7,None]}],
                 scoring='accuracy',
                 cv=2)

scores = cross_val_score(gs,X_train,y_train,
                        scoring='accuracy',cv=5)

np.mean(scores),np.std(scores)
# output:(0.9340659340659341, 0.015540808377726326)
```

> ä»ä»¥ä¸Šä»£ç çš„è¿è¡Œç»“æœä¸­å¯ä»¥çœ‹å‡ºSVM(97.4 percent) è¦æ¯”decision tree(93.4 percent) çš„ç»“æœè¦å¥½ã€‚å› æ­¤å¯¹å½“å‰æ•°æ®é›†æ¥è¯´é€‰æ‹©SVMä¼šæ›´å¥½ã€‚

> GridSearchCV ä¸­çš„accuracy æ˜¯ç”¨äºæ¯”è¾ƒå“ªä¸ªå‚æ•°ç»„åˆèƒ½å¾—åˆ°æ›´å¥½çš„æ¨¡å‹ï¼›è€Œcross_val_score ä¸­çš„scoringåˆ™å¯è®¤ä¸ºæ˜¯é€šè¿‡fitå¾—å‡ºçš„test set çš„ç»“æœã€‚


## Looking at different performance evaluation metrics

ä»…ä»…ä½¿ç”¨accuracyä½œä¸ºè¯„ä¼°æ¨¡å‹å¥½åæ˜¯ä¸å¤Ÿçš„ã€‚å› ä¸ºæœ‰æ—¶æ•°æ®æ˜¯ä¸å‡è¡¡çš„ã€‚æ•´ä½“æ¥è¯´å¯¹äºæ¨¡å‹çš„è¯„ä¼°éœ€è¦ç”¨åˆ°confusion matrixï¼ŒA confusion matrix is simply a square matrix that reports the counts of the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions of a classifierã€‚å¦‚ä¸‹ï¼š

![confusion matrix](/assets/images/python_ml/06_conf_matrix.png)

æˆ‘ä»¬çš„ç›®çš„æ˜¯è®©TPå’ŒTNå°½å¯èƒ½çš„å¤§ï¼Œè€ŒFNï¼ŒFPå°½å¯èƒ½çš„å°ã€‚

æˆ‘ä»¬ä¸‹é¢ä½¿ç”¨sklearnæ±‚å‡º[confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion_matrix#sklearn.metrics.confusion_matrix)ï¼Œå¹¶ç”¨seabornä¸­çš„[heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) ç”»å‡ºå›¾æ¥

```python
from sklearn.metrics import confusion_matrix
pipe_svc.fit(X_train,y_train)

y_pred = pipe_svc.predict(X_test)
confmat = confusion_matrix(y_true=y_test,y_pred=y_pred)

import seaborn as sns

sns.heatmap(confmat,
            center=0, 
            annot = True,
            cmap='RdYlGn', 
            linewidths=0.2)
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()
```

![heatmap of confusion matrix](/assets/images/python_ml/heatmap_conf.png)

> åœ¨heatmapä¸­é€šè¿‡ä½¿ç”¨`xticklabels, yticklabels`æ¥è®¾ç½®xè½´å’Œyè½´çš„åˆ»åº¦åç§°ã€‚ä½¿ç”¨plt.xlabel and plt.ylabelæ¥è®¾ç½®å›¾ä¸­xè½´å’Œè½´çš„æ ‡ç­¾ã€‚å…·ä½“çš„ä½¿ç”¨ç”¨æ³•å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ã€‚